# Glossário ETL

1. ETL (Extract, Transform, Load): Processo de extrair dados de várias fontes, transformá-los em um formato utilizável e carregá-los em um sistema de destino.
2. Dados: Informações brutas, fatos e estatísticas coletados para análise e tomada de decisão.
3. Extração: O processo de obter dados de várias fontes, como bancos de dados, arquivos e sistemas externos.
4. Transformação: O processo de converter dados extraídos em um formato consistente e adequado para análise.
5. Carregamento: O processo de inserir os dados transformados em um sistema de destino, como um data warehouse ou banco de dados.
6. Fonte de Dados: Sistemas ou locais de onde os dados são extraídos, como bancos de dados, arquivos ou APIs.
7. Destino de Dados: O sistema ou local onde os dados transformados são carregados, como um data warehouse ou banco de dados.
8. Data Warehouse: Um repositório central que armazena dados de várias fontes para análise e relatórios.
9. Limpeza de Dados: O processo de identificar e corrigir ou remover dados incorretos, incompletos ou duplicados.
10. Validação de Dados: O processo de verificar a precisão, consistência e qualidade dos dados.
11. Mapeamento de Dados: O processo de definir como os dados de origem correspondem aos dados de destino.
12. Integração de Dados: Combinar dados de diferentes fontes em uma visão unificada e consistente.
13. Big Data: Conjuntos de dados extremamente grandes e complexos que requerem processamento especial.
14. Processamento em Lote: Processar grandes volumes de dados de uma só vez, geralmente em intervalos regulares.
15. Processamento em Tempo Real: Processar dados imediatamente após a coleta, permitindo análises e decisões instantâneas.
16. Arquitetura de ETL: O design e a estrutura de um sistema ETL, incluindo componentes e fluxo de dados.
17. Ferramentas de ETL: Software especializado usado para projetar, construir e gerenciar processos de ETL.
18. Agendamento: Configurar processos de ETL para serem executados automaticamente em horários específicos.
19. Monitoramento: Acompanhar o desempenho e a integridade dos processos de ETL para garantir a confiabilidade dos dados.
20. Metadados: Informações que descrevem os dados, como tipo, formato, origem e relacionamentos.
21. Data Governance: Conjunto de políticas e procedimentos para gerenciar a disponibilidade, usabilidade, integridade e segurança dos dados.
22. Data Quality: O grau em que os dados atendem aos requisitos e expectativas para uso pretendido.
23. Data Profiling: Analisar os dados para entender suas características, estrutura e qualidade.
24. Data Lineage: Documentar a origem, o movimento e as transformações dos dados ao longo do tempo.
25. Conformidade de Dados: Garantir que os dados estejam em conformidade com regulamentos, políticas e padrões.
26. Segurança de Dados: Proteger os dados contra acesso não autorizado, uso indevido e perda.
27. Privacidade de Dados: Proteger informações pessoalmente identificáveis e

garantir o uso ético dos dados.

1. Scalability: A capacidade de um sistema ETL de lidar com volumes crescentes de dados e demandas de processamento.
2. Performance: A velocidade e eficiência com que um sistema ETL processa e entrega dados.
3. Cloud: Usar recursos e serviços de computação baseados na Internet para armazenar, gerenciar e processar dados.
4. Automação: Usar software e scripts para automatizar tarefas repetitivas nos processos de ETL.
5. Conectores: Componentes de software que permitem que sistemas ETL se conectem e acessem diferentes fontes e destinos de dados.
6. Transformações: Operações aplicadas aos dados durante o estágio de transformação, como filtragem, agregação e mesclagem.
7. Data Enrichment: Aprimorar os dados com informações adicionais de fontes externas para fornecer contexto e insights.
8. Data Cleansing: Identificar e corrigir registros de dados imprecisos, incompletos ou duplicados.
9. Data Standardization: Converter dados em um formato consistente para facilitar a integração e a análise.
10. Data Deduplication: Remover registros duplicados dos dados para melhorar a precisão e a eficiência do armazenamento.
11. Data Masking: Ocultar ou ofuscar informações confidenciais para proteger a privacidade e cumprir os regulamentos.
12. Change Data Capture (CDC): Identificar e capturar alterações nos dados de origem para atualizar incrementalmente os dados de destino.
13. Data Replication: Copiar dados de uma fonte para outra para melhorar a disponibilidade, o desempenho e a resiliência.
14. Data Partitioning: Dividir grandes conjuntos de dados em partes menores e mais gerenciáveis para processamento eficiente.
15. Data Archiving: Mover dados inativos ou raramente acessados para armazenamento de longo prazo para economizar espaço e melhorar o desempenho.
16. Data Lake: Um repositório que armazena dados brutos em seu formato nativo para análise exploratória e casos de uso futuros.
17. Data Catalog: Uma ferramenta que ajuda a descobrir, entender e governar ativos de dados em uma organização.
18. Data Streaming: Processar e analisar dados em tempo real à medida que são gerados ou recebidos.
19. Data Virtualization: Criar uma camada de abstração sobre dados de diferentes fontes para fornecer uma visão unificada e em tempo real.
20. Data Quality Assessment: Avaliar a qualidade dos dados com base em critérios como precisão, integridade, consistência e pontualidade.
21. Data Quality Monitoring: Monitorar continuamente a qualidade dos dados para identificar e resolver problemas à medida que surgem.
22. Data Lineage Tracking: Rastrear a origem, o movimento e as transformações dos dados para entender seu fluxo e garantir a conformidade.
23. Data Pipeline: Uma série de processos que movem e transformam dados de fontes para destinos, incluindo extração, transformação e carregamento.