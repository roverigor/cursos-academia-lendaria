# Aula 2.1 - Web Scraping Inteligente: Extraia Dados de Qualquer Site

**M√≥dulo:** 2 - Arsenal Pr√°tico  
**Dura√ß√£o:** 15 minutos  
**N√≠vel Bloom:** Apply  
**Instrutor:** Jos√© Carlos Amorim

---

## üéØ GOAL (O que voc√™ vai conseguir)

Ao final desta aula, voc√™ vai criar um **script de web scraping** que coleta dados de sites p√∫blicos automaticamente - tipo extrair lista de 500 empresas de um diret√≥rio com telefone, email e site em 2 minutos.

**Resultado concreto:** Planilha CSV com 50+ empresas coletadas de diret√≥rio p√∫blico, pronta para prospec√ß√£o.

---

## üìç POSITION (Onde voc√™ est√° agora)

Voc√™ j√° tem primeira automa√ß√£o rodando (organizar Downloads). Agora vamos para algo mais poderoso: **coletar dados da internet automaticamente**.

**Sabe aquele momento** que voc√™ precisa de uma lista de leads - tipo 500 franquias brasileiras com contato - e suas op√ß√µes s√£o:

1. **Comprar lista** (R$500-2000, dados desatualizados)
2. **Copiar manualmente** (40 horas navegando sites, Ctrl+C, Ctrl+V)
3. **Contratar estagi√°rio** (R$1500/m√™s + treinamento)

**Hoje voc√™ aprende a op√ß√£o 4:** Script que faz isso em 5 minutos. De gra√ßa. Atualizado sempre que rodar.

**Web scraping √© tipo Ctrl+C, Ctrl+V autom√°tico da internet.** Voc√™ ensina o computador onde est√° a informa√ß√£o, ele coleta tudo sozinho.

---

## üîÑ STEPS (Como chegar l√°)

### PASSO 1: √âtica & Legalidade PRIMEIRO (3min)

**ANTES de scrape qualquer coisa, voc√™ PRECISA entender as regras.**

#### O QUE √â WEB SCRAPING

Scraping = coletar dados p√∫blicos de sites automaticamente via c√≥digo.

**√â legal?** Depende.

**‚úÖ LEGAL (pode fazer):**
- Dados p√∫blicos (sem login, sem paywall)
- Para uso pr√≥prio ou pesquisa
- Respeitando robots.txt
- Com rate limiting (n√£o derrubar site)
- Sites sem proibi√ß√£o expl√≠cita nos Termos de Uso

**‚ùå ILEGAL/ANTI√âTICO (N√ÉO fazer):**
- Dados privados (atr√°s de login/paywall)
- Violar Termos de Servi√ßo expl√≠citos
- Derrubar site com requisi√ß√µes excessivas
- Revender dados sem permiss√£o
- Informa√ß√µes pessoais protegidas (LGPD)

#### VERIFICAR ROBOTS.TXT

Todo site s√©rio tem arquivo `robots.txt` dizendo o que bots podem/n√£o podem acessar.

**Como verificar:**
```
https://site.com.br/robots.txt
```

**Exemplo:** https://franquias.portaldofranchising.com.br/robots.txt

**Se ver:**
```
User-agent: *
Disallow: /admin/
Disallow: /private/
```
= Pode scrapear tudo EXCETO /admin/ e /private/ ‚úÖ

**Se ver:**
```
User-agent: *
Disallow: /
```
= Site pro√≠be scraping completamente ‚ùå

**REGRA DE OURO:** Se robots.txt pro√≠be, respeita. Busca alternativa ou API oficial.

#### RATE LIMITING

N√£o bombardeie sites com 1000 requisi√ß√µes por segundo. Isso √© ataque DDoS.

**Boas pr√°ticas:**
- 1 requisi√ß√£o por segundo (conservador)
- 5-10 requisi√ß√µes por segundo (razo√°vel)
- Pausa de 2-5seg entre p√°ginas

Seu script vai ter `time.sleep(1)` entre requisi√ß√µes. Sempre.

---

### PASSO 2: Anatomia de Uma P√°gina Web (2min)

#### HTML = ESTRUTURA DA P√ÅGINA

Quando voc√™ abre site no navegador, por baixo tem c√≥digo HTML.

**Exemplo simples:**
```html
<div class="empresa">
  <h2 class="nome">Franquia Exemplo</h2>
  <p class="telefone">(11) 9999-9999</p>
  <a class="site" href="https://exemplo.com">Site</a>
</div>
```

**Web scraping = encontrar esses "containers" e extrair texto de dentro.**

#### INSPECIONAR ELEMENTO (Ferramenta Essencial)

**Como ver estrutura HTML:**

1. Abre site no Chrome/Firefox
2. Clique direito em elemento (ex: nome da empresa)
3. "Inspecionar" ou "Inspect Element"
4. Aparecer c√≥digo HTML destacando elemento

**Voc√™ vai usar isso para saber ONDE est√° o dado que quer.**

Exemplo: "Ah, o nome da empresa est√° dentro de `<h2 class='empresa-nome'>`"

Essa informa√ß√£o vai no seu script.

---

### PASSO 3: Primeiro Script de Scraping (6min)

#### EXEMPLO REAL: Scrapear Diret√≥rio de Empresas

Vamos criar script que:
1. Acessa p√°gina de diret√≥rio p√∫blico
2. Extrai nome, cidade, site de cada empresa
3. Salva tudo em CSV

**PROMPT PARA CLAUDE:**

```
Preciso de um script Python para web scraping com estas specs:

URL alvo: [COLE URL DO DIRET√ìRIO QUE QUER SCRAPEAR]

Dados a extrair de cada empresa listada:
- Nome (est√° em tag <h3 class="company-name">)
- Cidade (est√° em <span class="location">)
- Site (est√° em <a class="website">)

Requisitos:
1. Usar BeautifulSoup + requests
2. Rate limiting: 1 req/segundo (time.sleep)
3. Salvar em CSV com headers (Nome, Cidade, Site)
4. Mostrar progresso (print a cada 10 empresas)
5. Error handling (se p√°gina n√£o carregar, skip)

O script deve processar apenas a primeira p√°gina (MVP).
```

**CLAUDE VAI GERAR:**

```python
import requests
from bs4 import BeautifulSoup
import csv
import time

# Configura√ß√µes
URL = "https://exemplo.com/diretorio"
OUTPUT_FILE = "empresas_coletadas.csv"

def scrape_diretorio():
    print("üöÄ Iniciando scraping...")
    
    # Requisi√ß√£o HTTP
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(URL, headers=headers)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar site: {e}")
        return
    
    # Parse HTML
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Encontrar todas empresas (ajuste seletor conforme site)
    empresas = soup.find_all('div', class_='empresa-card')
    
    print(f"üìä {len(empresas)} empresas encontradas")
    
    # Coletar dados
    dados = []
    for idx, empresa in enumerate(empresas, 1):
        try:
            nome = empresa.find('h3', class_='company-name').text.strip()
            cidade = empresa.find('span', class_='location').text.strip()
            site_tag = empresa.find('a', class_='website')
            site = site_tag['href'] if site_tag else 'N/A'
            
            dados.append({
                'Nome': nome,
                'Cidade': cidade,
                'Site': site
            })
            
            if idx % 10 == 0:
                print(f"‚úÖ {idx} empresas processadas...")
                
        except AttributeError:
            print(f"‚ö†Ô∏è Empresa {idx}: dados incompletos, pulando...")
            continue
        
        # Rate limiting
        time.sleep(1)
    
    # Salvar CSV
    if dados:
        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['Nome', 'Cidade', 'Site'])
            writer.writeheader()
            writer.writerows(dados)
        
        print(f"üéâ {len(dados)} empresas salvas em {OUTPUT_FILE}")
    else:
        print("‚ùå Nenhum dado coletado")

if __name__ == "__main__":
    scrape_diretorio()
```

#### INSTALAR DEPEND√äNCIAS

Antes de rodar, precisa instalar bibliotecas:

```bash
pip3 install requests beautifulsoup4
```

(S√≥ faz isso uma vez. Depois ficam instaladas.)

#### AJUSTAR SELETORES CSS

**CR√çTICO:** Voc√™ precisa ajustar seletores (`class='company-name'`) pro site real que vai scrapear.

**Como fazer:**
1. Inspeciona elemento no site
2. V√™ class/id do container
3. Ajusta no c√≥digo: `soup.find('tag', class_='class-real')`

**Exemplo real (Portal do Franchising):**
```python
nome = empresa.find('h2', class_='name').text.strip()
cidade = empresa.find('span', class_='city').text.strip()
```

**Sempre testa com print antes de salvar CSV:**
```python
print(f"Nome: {nome}, Cidade: {cidade}")
```

#### EXECUTAR

```bash
python3 scrape_empresas.py
```

**Output esperado:**
```
üöÄ Iniciando scraping...
üìä 50 empresas encontradas
‚úÖ 10 empresas processadas...
‚úÖ 20 empresas processadas...
...
üéâ 50 empresas salvas em empresas_coletadas.csv
```

**Abre CSV ‚Üí Planilha com 50 empresas coletadas!** üéâ

---

### PASSO 4: Scraping Multi-P√°ginas (2min)

Seu script b√°sico coleta s√≥ primeira p√°gina. E se site tem 20 p√°ginas?

**PROMPT PARA CLAUDE:**

```
Ajusta o script anterior para scrapear m√∫ltiplas p√°ginas.

Site usa pagina√ß√£o tipo: 
- P√°gina 1: https://site.com/diretorio?page=1
- P√°gina 2: https://site.com/diretorio?page=2
- etc

Requisitos:
1. Loop de p√°gina 1 at√© 10 (ou at√© n√£o ter mais dados)
2. Rate limiting 2 segundos entre p√°ginas
3. Consolidar todos dados em √∫nico CSV
```

**CLAUDE VAI ADICIONAR:**

```python
def scrape_multiplas_paginas(max_pages=10):
    todos_dados = []
    
    for page in range(1, max_pages + 1):
        print(f"\nüìÑ Processando p√°gina {page}...")
        url = f"https://site.com/diretorio?page={page}"
        
        # [c√≥digo de scraping aqui]
        
        if not empresas:  # Se p√°gina vazia, para
            print("‚úã Sem mais dados. Parando.")
            break
        
        todos_dados.extend(dados_pagina)
        time.sleep(2)  # Pausa entre p√°ginas
    
    return todos_dados
```

**Agora coleta centenas/milhares de empresas automaticamente.** üöÄ

---

### PASSO 5: Casos de Uso Reais (2min)

#### EXEMPLO 1: Prospec√ß√£o B2B

**Tarefa:** Coletar 500 franquias BR para vender consultoria

**Script:**
- Scrapea Portal do Franchising
- Filtra por segmento (alimenta√ß√£o, sa√∫de, etc)
- Extrai: Nome, Site, Telefone, Email (se p√∫blico)
- CSV ‚Üí Importa no CRM ‚Üí Campanha cold email

**ROI:** 40 horas manual ‚Üí 10 minutos script

#### EXEMPLO 2: Monitoramento de Pre√ßos

**Tarefa:** Acompanhar pre√ßos de 20 concorrentes toda semana

**Script:**
- Scrapea p√°ginas de produtos espec√≠ficos
- Extrai pre√ßo atual
- Salva hist√≥rico em planilha
- Alerta se pre√ßo mudou >10%

**ROI:** 2 horas/semana manual ‚Üí 30 segundos script

#### EXEMPLO 3: Pesquisa de Mercado

**Tarefa:** Coletar reviews de produto em marketplace

**Script:**
- Scrapea reviews de 100 produtos similares
- Extrai rating, texto, data
- An√°lise: palavras mais comuns em reviews negativos
- Insights para melhorar produto

**ROI:** 20 horas pesquisa manual ‚Üí 1 hora script + an√°lise

---

## üéì S√çNTESE (Takeaways)

**O que voc√™ aprendeu:**

1. **Web scraping = Ctrl+C, Ctrl+V autom√°tico**
   - Coleta dados p√∫blicos de sites em minutos
   - Economia: 10-40 horas de trabalho manual

2. **√âtica PRIMEIRO:**
   - ‚úÖ Dados p√∫blicos, robots.txt OK, rate limiting
   - ‚ùå Dados privados, ToS violados, DDoS

3. **Stack t√©cnico:**
   - Requests (fazer requisi√ß√µes HTTP)
   - BeautifulSoup (parse HTML)
   - CSV (salvar dados)

4. **Processo:**
   - Inspecionar elemento ‚Üí Identificar seletores CSS
   - Prompt Claude com specs claras
   - Ajustar seletores pro site real
   - Testar com print ‚Üí Executar ‚Üí CSV

5. **Casos de uso:** Prospec√ß√£o B2B, monitoramento pre√ßos, pesquisa de mercado

**Skills dominadas:**
- ‚úÖ Verificar robots.txt
- ‚úÖ Inspecionar elemento (Chrome DevTools)
- ‚úÖ Ajustar seletores CSS
- ‚úÖ Scraping single-page
- ‚úÖ Scraping multi-p√°ginas
- ‚úÖ Rate limiting √©tico

---

## üöÄ PR√ìXIMOS PASSOS

**Desafio pr√°tico:**

Escolhe UM site p√∫blico que tenha dados √∫teis pro seu neg√≥cio:
- Diret√≥rio de empresas
- Marketplace com produtos
- Site de not√≠cias (headlines)
- Qualquer coisa P√öBLICA

Cria script de scraping seguindo modelo desta aula.

**Se travar:** Aula 3.1 (debugging) vai te salvar. Scraping sempre d√° erro na primeira tentativa (estrutura HTML muda, site bloqueia, seletor errado). √â normal.

**Pr√≥xima aula (2.2):**
"Processamento em Massa: 1000 Documentos em 1 Comando"

Coletou 500 empresas? Agora precisa processar 500 PDFs, extrair dados, consolidar. Vamos escalar.

**Scraping √© coleta. Processamento √© transforma√ß√£o. Voc√™ vai dominar os dois.** üí™

---

## üí≠ REFLEX√ÉO FINAL

**Porque no fundo...**

Web scraping √© poder.

Poder de coletar dados que levaria DIAS manualmente.

Poder de monitorar mercado em tempo real.

Poder de validar ideias com dados reais, n√£o achismos.

**Mas com grande poder vem grande responsabilidade** (tio Ben tinha raz√£o).

Use √©tico. Respeite robots.txt. Rate limiting sempre. Dados p√∫blicos apenas.

**Voc√™ agora tem superpoder de coletar dados da internet. Use com sabedoria.** üöÄ

---

**Nos vemos na pr√≥xima aula. Hora de processar dados em massa.**

---

**Instrutor:** Jos√© Carlos Amorim  
**Dura√ß√£o:** 15 minutos  
**Framework:** GPS + Did√°tica Lend√°ria + ESPIRAL EXPANSIVA  
**Voice Fidelity Target:** 85-90% (MMOS)

