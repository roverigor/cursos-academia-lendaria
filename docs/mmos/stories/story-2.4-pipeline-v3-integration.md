# Story 2.4: Pipeline Integration with Database v3.0.0

**Story ID:** MMOS-2.4
**Epic:** Epic 2 - Database & Backend Foundation
**Priority:** HIGH
**Status:** üìã Ready for Development
**Estimate:** 2-3 days
**Scope:** MMOS pipeline only (InnerLens in Story 4.2)
**Dependencies:**
- Story 2.1 ‚úÖ (Database schema design)
- Story 2.2 ‚úÖ (Specialization taxonomy population)
- Story 2.3 ‚úÖ (Core minds population)
- Story 3.1 ‚úÖ (Pilot migration - for testing with sam_altman)

---

## User Story

**As a** MMOS developer
**I want** the collection pipeline to populate the database v3.0.0 directly
**So that** new mind processing automatically creates sources, fragments, analysis, and proficiency scores

---

## Context

Currently, the MMOS pipeline generates YAML files:
- `sources_master.yaml`
- `cognitive-spec.yaml`
- `discovery_report.yaml`

But these files are **not connected to the database v3.0.0** (MMOS + InnerLens + Specialization).

We need to update the pipeline to:
1. Still generate YAML files (backward compatibility)
2. **Also populate the database in real-time**
3. Generate fragments from cognitive analysis
4. **[REMOVED]** ~~Score proficiencies~~ (moved to Story 4.3 - manual pilot first)
5. **[REMOVED]** ~~Create fragment tags~~ (moved to Story 4.3)
6. Support re-processing existing minds (UPDATE mode)

**Out of Scope:**
- InnerLens 120-trait analysis (Story 4.2)
- Automated proficiency scoring (Story 4.3)
- Fragment tags generation (Story 4.3)

---

## Acceptance Criteria

### AC0: Re-processing Strategy

**Decision:** Support both fresh insert and update modes

**Modes:**
- `--mode fresh`: Delete existing data first, then insert
- `--mode update`: Update existing records, insert new ones (ON CONFLICT)
- `--mode skip`: Skip if data already exists (default safe mode)

**Implementation:**
```bash
# Fresh re-process (clean slate)
bash scripts/pipeline/db-integration-v3.sh --mind sam_altman --mode fresh

# Update existing data
bash scripts/pipeline/db-integration-v3.sh --mind sam_altman --mode update

# Skip if exists (safe default)
bash scripts/pipeline/db-integration-v3.sh --mind sam_altman --mode skip
```

### AC1: Source Population Module

**File:** `scripts/pipeline/populate-sources.js`

**Function:**
- Reads `sources_master.yaml` from a mind directory
- Parses YAML sources list
- Inserts into `sources` table with proper foreign keys
- Maps source types: `blog ‚Üí blog`, `youtube ‚Üí video`, `pdf ‚Üí document`
- Handles duplicates (ON CONFLICT)

**Success:**
```bash
node scripts/pipeline/populate-sources.js \
  --mind sam_altman \
  --file outputs/minds/sam_altman/sources/sources_master.yaml \
  --db SQLite legado (migrado para Supabase em 2025-10)

# Output:
# ‚úì Inserted 35 new sources
# ‚úì Updated 3 existing sources
# ‚úì Total sources for sam_altman: 38
```

### AC2: Fragments Extraction Module

**File:** `scripts/pipeline/extract-fragments.js`

**Function:**
- Reads `cognitive-spec.yaml`
- Parses all 8 layers
- Extracts every `evidence` block with quotes
- Creates fragment for each evidence piece
- Links fragment to source via source_id
- Generates UUID for each fragment
- Sets cognitive_theme from layer context

**Fragment types mapping:**
- Blog evidence ‚Üí `written_thought`
- Interview evidence ‚Üí `dialogue`
- Testimony evidence ‚Üí `statement`
- Observation from analysis ‚Üí `meta_pattern`

**Success:**
```bash
node scripts/pipeline/extract-fragments.js \
  --mind sam_altman \
  --cognitive-spec outputs/minds/sam_altman/analysis/cognitive-spec.yaml \
  --db SQLite legado (migrado para Supabase em 2025-10)

# Output:
# ‚úì Extracted 142 fragments from cognitive-spec
# ‚úì Layer 1: 12 fragments
# ‚úì Layer 2: 15 fragments
# ‚úì Layer 3: 18 fragments
# ‚úì Layer 4: 22 fragments
# ‚úì Layer 5: 14 fragments
# ‚úì Layer 6: 25 fragments
# ‚úì Layer 7: 16 fragments
# ‚úì Layer 8: 20 fragments
```

### AC3: Analysis Import Module

**File:** `scripts/pipeline/import-analysis.js`

**Function:**
- Reads complete `cognitive-spec.yaml`
- Stores entire YAML as JSON in `analysis` table
- Sets `analysis_type = 'dna_mental_cognitive_architecture'`
- Extracts confidence scores per layer
- Sets overall confidence_score

**Success:**
```bash
node scripts/pipeline/import-analysis.js \
  --mind sam_altman \
  --file outputs/minds/sam_altman/analysis/cognitive-spec.yaml \
  --db SQLite legado (migrado para Supabase em 2025-10)

# Output:
# ‚úì Imported DNA Mental‚Ñ¢ analysis
# ‚úì Overall confidence: 0.90
# ‚úì Layers analyzed: 8
# ‚úì Total evidence points: 142
```

### AC4: Validation Module

**File:** `scripts/pipeline/validate-integration.js`

**Function:**
- Validates referential integrity
- Checks data completeness
- Identifies orphaned records
- Generates validation report

**Checks:**
1. All fragments link to valid source_id
2. All fragments link to valid mind_id
3. All sources link to valid mind_id
4. All analysis entries have valid mind_id
5. No orphaned records
6. Fragment content is valid JSON
7. Analysis content is valid JSON

**Success:**
```bash
node scripts/pipeline/validate-integration.js \
  --mind sam_altman \
  --db SQLite legado (migrado para Supabase em 2025-10)

# Output:
# ‚úì Checking referential integrity...
# ‚úì All 38 sources link to valid mind
# ‚úì All 142 fragments link to valid source
# ‚úì All 142 fragments link to valid mind
# ‚úì Analysis entry links to valid mind
# ‚úì No orphaned records found
# ‚úì All fragment content is valid JSON
# ‚úì Analysis content is valid JSON
#
# üìä Validation Summary:
# ‚Ä¢ Total checks: 7
# ‚Ä¢ Passed: 7
# ‚Ä¢ Failed: 0
# ‚Ä¢ Status: ‚úÖ PASS
```

### AC5: Integration Script

**File:** `scripts/pipeline/db-integration-v3.sh`

**Function:**
- Orchestrates all 4 modules sequentially
- Validates database state before/after
- Generates integration report
- Supports modes: `full`, `sources-only`, `analysis-only`
- Supports re-processing: `fresh`, `update`, `skip`

**Success:**
```bash
bash scripts/pipeline/db-integration-v3.sh --mind sam_altman --mode full

# Output:
# [INFO] Starting database integration for: sam_altman
# [INFO] Mode: full
# [SUCCESS] Found mind in database (ID: 22)
#
# [INFO] Phase 1: Populating sources...
# [SUCCESS] Sources populated successfully
#
# [INFO] Phase 2: Extracting fragments from analysis...
# [SUCCESS] Fragments extracted successfully
#
# [INFO] Phase 3: Importing DNA Mental‚Ñ¢ analysis...
# [SUCCESS] Analysis imported successfully
#
# [INFO] Phase 4: Validating integration...
# üìä Integration Report for: sam_altman
#
# ‚Ä¢ Sources: 38
# ‚Ä¢ Fragments: 142
# ‚Ä¢ Analysis entries: 1
# ‚Ä¢ Validation: ‚úÖ PASS
#
# [SUCCESS] Database integration complete for: sam_altman
```

### AC6: Pipeline Update Task

**File:** `.claude/commands/MMOS/tasks/cognitive-analysis.md`

**Update:** Add database integration step at the end:

```markdown
## Passos
1. Carregar notas de discovery e outputs do ETL
2. Para cada camada selecionada, preencher se√ß√µes em `analysis/layers/{layer}.md`
3. Gerar `cognitive-spec.yaml` completo
4. **[NEW]** Executar integra√ß√£o com banco de dados:
   ```bash
   bash scripts/pipeline/db-integration-v3.sh \
     --mind {mind_slug} \
     --mode analysis-only
   ```
5. Registrar lacunas e evid√™ncias adicionais necess√°rias
6. Atualizar `STATUS.md` com progresso da an√°lise
```

---

## Success Metrics

**Per Mind Integration:**
- ‚úÖ All sources from sources_master.yaml imported (100%)
- ‚úÖ 50-200 fragments extracted from cognitive-spec
- ‚úÖ 1 DNA Mental‚Ñ¢ analysis stored
- ‚úÖ 100% referential integrity validated
- ‚úÖ Re-processing modes working (fresh/update/skip)

**Code Quality:**
- ‚úÖ All scripts have error handling
- ‚úÖ All scripts support dry-run mode
- ‚úÖ All scripts generate detailed logs
- ‚úÖ All scripts are idempotent (safe to re-run)

**Performance:**
- ‚è±Ô∏è Source population: <30 seconds per mind
- ‚è±Ô∏è Fragment extraction: <2 minutes per mind
- ‚è±Ô∏è Analysis import: <30 seconds per mind
- ‚è±Ô∏è Validation: <30 seconds per mind
- ‚è±Ô∏è Total integration: <5 minutes per mind

---

## Implementation Plan

### Day 1 (3-4 hours)

**Morning:**
1. Create `populate-sources.js` (1 hour)
2. Create `import-analysis.js` (45 min)
3. Create `extract-fragments.js` (1.5 hours)

**Afternoon:**
4. Create `validate-integration.js` (45 min)

### Day 2 (2-3 hours)

**Morning:**
5. Create `db-integration-v3.sh` orchestrator (1 hour)
6. Test with sam_altman (pilot validation) (1 hour)

**Afternoon:**
7. Fix bugs and edge cases (30-60 min)
8. Update MMOS task files (30 min)

### Day 3 (OPTIONAL - if issues found)

**Extended Testing:**
9. Run integration on 3 additional minds (1 hour)
10. Performance optimization if needed (1 hour)

---

## Technical Design

### Database Schema Mapping

```
YAML Files                    ‚Üí  Database Tables
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
sources_master.yaml           ‚Üí  sources
  ‚îú‚îÄ id                       ‚Üí    id (UUID or slug)
  ‚îú‚îÄ type                     ‚Üí    source_type
  ‚îú‚îÄ title                    ‚Üí    title
  ‚îú‚îÄ url                      ‚Üí    url
  ‚îú‚îÄ file_path                ‚Üí    file_path
  ‚îú‚îÄ word_count               ‚Üí    word_count
  ‚îî‚îÄ confidence               ‚Üí    confidence

cognitive-spec.yaml           ‚Üí  fragments + analysis
  ‚îú‚îÄ layer_N.evidence         ‚Üí    fragments
  ‚îÇ    ‚îú‚îÄ source             ‚Üí      source_id (FK)
  ‚îÇ    ‚îú‚îÄ quote              ‚Üí      content (JSON)
  ‚îÇ    ‚îî‚îÄ description        ‚Üí      why_significant
  ‚îú‚îÄ full spec               ‚Üí    analysis (JSON)
  ‚îî‚îÄ confidence              ‚Üí    confidence_score

**Note:** Proficiency scoring moved to Story 4.3
```

### Fragment Content JSON Structure

```json
{
  "text": "Trust the exponential, be patient, and be pleasantly surprised.",
  "context": "Layer 1 - Sensory Inputs - Preferred Inputs",
  "source_context": "Blog post: How To Be Successful",
  "layer": 1,
  "layer_name": "Sensory Inputs & Context"
}
```

### Re-processing Strategy

```
Mode: fresh
1. DELETE FROM fragments WHERE mind_id = ?
2. DELETE FROM sources WHERE mind_id = ?
3. DELETE FROM analysis WHERE mind_id = ?
4. INSERT new data

Mode: update
1. INSERT ... ON CONFLICT(unique_key) DO UPDATE

Mode: skip
1. Check if data exists
2. Skip if found
3. Insert only new records
```

---

## Risks & Mitigations

**Risk 1: YAML parsing errors**
- **Mitigation:** Use robust YAML parsers (js-yaml), validate schema, graceful degradation

**Risk 2: Fragment duplication**
- **Mitigation:** Content hashing, check before insert, unique constraints

**Risk 3: Re-processing data corruption**
- **Mitigation:** Always backup before `--mode fresh`, transaction rollback on error

**Risk 4: Performance degradation**
- **Mitigation:** Batch inserts, database indexes, async processing

**Risk 5: Breaking existing pipeline**
- **Mitigation:** Backward compatibility (keep YAML generation), test with sam_altman pilot

---

## Testing Strategy

### Unit Tests
- ‚úÖ YAML parsing (malformed files)
- ‚úÖ Fragment extraction (various layer structures)
- ‚úÖ Re-processing modes (fresh/update/skip)
- ‚úÖ Database transactions (rollback on error)

### Integration Tests
- ‚úÖ Full pipeline with sam_altman
- ‚úÖ Validate referential integrity
- ‚úÖ Check data completeness
- ‚úÖ Performance benchmarks

### Edge Cases
- ‚úÖ Mind with 0 sources
- ‚úÖ Mind with empty cognitive-spec
- ‚úÖ Mind with only 1-2 evidence pieces
- ‚úÖ Mind with partial cognitive-spec (layers 1-4 only)
- ‚úÖ Re-running integration (all 3 modes)

---

## Next Steps After Completion

1. ‚úÖ Story 2.4 complete: MMOS pipeline integrated with database
2. ‚Üí **Story 3.1.1**: Full rollout to remaining 27 minds
3. ‚Üí **Epic 4 Story 4.1**: InnerLens 120-trait analysis integration
4. ‚Üí **Epic 4 Story 4.2**: Automated proficiency scoring + fragment tags
5. ‚Üí **Epic 4 Story 4.3**: Real-time sync (watch files ‚Üí auto-update DB)

---

**Created:** October 12, 2025
**Last Updated:** October 13, 2025 (Moved from Story 4.1 to Epic 2)
**Author:** Mary (Business Analyst) + Dev Team
**Epic:** Epic 2 - Database & Backend Foundation
**Story Points:** 8 (Fibonacci - reduced after scope cut)

---

## Checklist

### Phase 1: Core Scripts
- [ ] populate-sources.js
- [ ] import-analysis.js (moved before fragments)
- [ ] extract-fragments.js
- [ ] validate-integration.js

### Phase 2: Integration
- [ ] db-integration-v3.sh
- [ ] Update MMOS tasks
- [ ] Test with sam_altman

### Phase 3: Validation
- [ ] Referential integrity check
- [ ] Data quality validation
- [ ] Performance benchmarks
- [ ] Documentation complete

### Phase 4: Rollout
- [ ] Process andrej_karpathy
- [ ] Process 5 more incomplete minds
- [ ] Full 28-mind validation
