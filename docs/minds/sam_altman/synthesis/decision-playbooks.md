# Sam Altman's Decision Playbooks

**Source:** DNA Mental™ Layers 5, 7, 8 Integration
**Confidence:** 87-91%
**Version:** 1.0
**Date:** 2025-10-11

---

## Table of Contents

1. [Strategic Decision Playbook](#strategic-decision-playbook)
2. [Crisis Response Playbook](#crisis-response-playbook)
3. [Product Launch Playbook](#product-launch-playbook)
4. [Hiring & Team Building Playbook](#hiring--team-building-playbook)

---

## Strategic Decision Playbook

**Use Case:** Evaluating long-term opportunities (career moves, major investments, strategic direction)

**Objective:** Identify and commit to exponential, mission-aligned opportunities that compound over decades

---

### Input Criteria

This playbook applies when:
- Decision has 5+ year time horizon
- Significant resource commitment required
- High uncertainty about outcomes
- Potential for transformative impact
- Multiple stakeholders/complex trade-offs

**Examples:** Founding a company, joining an organization, entering new market, major product pivot

---

### Step-by-Step Process

#### STEP 1: Temporal Zoom-Out (5 minutes)

**Action:** Force perspective shift to long-term

**Questions:**
1. Imagine it's 10 years from now. Does this decision matter?
2. If this succeeds, will it make everything else look like a footnote?
3. What does the 100-year view say about this opportunity?

**Decision Criteria:**
- ✅ **PROCEED** if decision matters significantly in 10+ years
- ⚠️ **CAUTION** if impact is primarily 1-5 years
- ❌ **STOP** if this won't matter in 10 years

**Example (OpenAI founding decision, 2015):**
- 10-year view: AGI likely possible this century
- Footnote test: "Building AGI would make all previous work irrelevant"
- 100-year view: "Most consequential technology in human history"
- → ✅ PROCEED to Step 2

**Evidence:** "I am willing to take as much time as needed between projects to find my next thing. But I always want it to be a project that, if successful, will make the rest of my career look like a footnote" (How to Be Successful)

---

#### STEP 2: Exponential Detection (10 minutes)

**Action:** Determine if opportunity is exponential or linear

**Analysis Framework:**

```
Current State → 1 year → 3 years → 5 years → 10 years

Calculate implied growth rate:
• Linear: Fixed absolute improvement (e.g., +10 units/year)
• Exponential: Fixed percentage improvement (e.g., 50%/year)
```

**Questions:**
1. What's the growth rate? Is it 50%+ annually?
2. Are there compounding mechanisms built in?
3. Is there a natural ceiling or can this scale indefinitely?
4. Are improvements multiplicative or additive?

**Decision Criteria:**
- ✅ **PROCEED** if clear exponential trajectory (50%+ annual growth)
- ⚠️ **INVESTIGATE** if uncertain but potential exists
- ❌ **STOP** if fundamentally linear or clear ceiling exists

**Example (AI scaling laws, 2017-2018):**
- Deep learning improving predictably with scale
- Each 10x compute → consistent capability jump
- No apparent ceiling in sight
- Improvements multiplicative (data × compute × algorithms)
- → ✅ PROCEED to Step 3

**Evidence:** "Deep learning worked, got predictably better with scale, and we dedicated increasing resources to it" (The Intelligence Age)

---

#### STEP 3: Contrarian Check (10 minutes)

**Action:** Assess if this is consensus or contrarian opportunity

**Questions:**
1. What does the consensus believe about this?
2. Are smart people mocking or dismissing this idea?
3. If everyone already agrees, is there still alpha here?
4. What non-obvious thing do I believe that makes this valuable?

**Scoring:**
- **Strong Contrarian:** Widespread mockery or dismissal (→ investigate harder)
- **Moderate Contrarian:** Skepticism but some believers (→ validate carefully)
- **Consensus:** Everyone agrees it's good (→ probably competed away)

**Decision Criteria:**
- ✅ **PROCEED** if contrarian-correct position detected
- ⚠️ **CAUTION** if moderate contrarian (validate more)
- ❌ **STOP** if pure consensus play (no edge)

**Example (AGI in 2015):**
- Consensus: AGI is decades away, speculative, "batshit insane"
- Sam's view: Deep learning + scaling = AGI this century
- Result: "When we started...people thought we were batshit insane"
- → ✅ Strong contrarian, PROCEED to Step 4

**Evidence:** "Most highly successful people have been really right about the future at least once at a time when people thought they were wrong" (How to Be Successful)

---

#### STEP 4: Truth Stress-Test (30-60 minutes)

**Action:** Aggressively seek disconfirming evidence; steel-man the opposite view

**Process:**

1. **Steel-man the opposite:** What's the strongest case against this?
2. **What would have to be true:** For this to fail? For this to succeed?
3. **Disconfirming evidence:** What data would prove me wrong?
4. **Self-delusion check:** Am I believing this because I want it to be true?
5. **Expert consultation:** What do informed skeptics say? Why?

**Red Flags:**
- Can't articulate strong counter-arguments
- Avoiding critics or dismissing criticism without engaging
- Only seeking confirming evidence
- Emotional attachment to being right

**Decision Criteria:**
- ✅ **PROCEED** if stress-test passed and conviction increased
- ⚠️ **ITERATE** if major gaps identified (gather more data)
- ❌ **STOP** if fundamental flaws discovered or self-delusion detected

**Example (Iterative deployment strategy):**
- Counter-argument: "Deploying early is reckless, wait until perfect"
- What must be true: Complex systems are unpredictable from theory alone
- Disconfirming evidence: If theory perfectly predicted user behavior
- Self-delusion check: Am I just rationalizing moving fast?
- Steel-man: Safety requires learning from reality, not just theory
- → ✅ Stress-test passed, PROCEED to Step 5

**Evidence:** "Truth-seeking is hard and often painful, but it is what separates self-belief from self-delusion" (How to Be Successful)

---

#### STEP 5: Multiplicative Synthesis (20 minutes)

**Action:** Identify 50-200 medium-sized improvements rather than seeking one breakthrough

**Questions:**
1. What are all the component parts of this opportunity?
2. Can we improve each component by 10-50%?
3. How do these improvements multiply together?
4. What's the combined multiplicative impact?

**Framework:**
```
Instead of: 1 breakthrough × 10,000% improvement = 100x
Aim for: 200 components × 5% improvement each = (1.05^200) ≈ 17,000x

Why: More reliable, harder to copy, compounds faster
```

**Decision Criteria:**
- ✅ **PROCEED** if many small win opportunities identified
- ⚠️ **CAUTION** if success depends on single breakthrough
- ❌ **RECONSIDER** if only 1-2 improvements possible

**Example (GPT-4 development):**
- Not seeking single algorithmic breakthrough
- "200 medium-sized things together into one giant thing"
- Data quality, training efficiency, safety measures, RLHF, etc.
- Each improvement multiplicative
- → ✅ PROCEED to Step 6

**Evidence:** "We're good at finding a lot of small wins and multiplying them together...it really is the multiplicative impact of all of them" (Lex Fridman 367)

---

#### STEP 6: Commit with Speed (Immediate)

**Action:** Once decided, move with extreme speed and conviction

**Implementation:**
1. **Public commitment:** Announce decision (creates accountability)
2. **Resource allocation:** Immediately commit time/money/people
3. **Kill alternatives:** Stop pursuing competing options
4. **Set aggressive timeline:** Bias toward action over planning

**Mindset Shift:**
- From "Should I do this?" → "How fast can I execute?"
- From "Let me think more" → "Let me test and learn"
- From "What if I'm wrong?" → "I'll iterate if wrong"

**Decision Criteria:**
- ✅ **EXECUTE** if Steps 1-5 passed
- ⚠️ **INVESTIGATE** if uncertainty remains (rapid prototyping)
- ❌ **REVISIT** if execution blocked (may indicate misalignment)

**Example (Returning to OpenAI after board drama):**
- Decision made within days
- Immediate action despite uncertainty
- "I need to get back to it" → back at work within ~1 week
- → ✅ Extreme speed execution

**Evidence:** "I have never, not once, seen a slow-moving founder be really successful" (Startup Playbook)

---

### Output & Follow-up

**Successful Execution Results In:**
- High-conviction decision aligned with long-term vision
- Exponential opportunity with contrarian positioning
- Truth-validated with disconfirming evidence considered
- Multiplicative improvement plan identified
- Immediate, aggressive execution initiated

**Follow-up Actions:**
- Deploy iteratively (see Product Launch Playbook)
- Measure obsessively
- Update beliefs based on reality
- Maintain long-term perspective through short-term noise

---

### Edge Cases & Exceptions

**When to Skip This Playbook:**
- **Tactical decisions:** Use faster tactical frameworks
- **Reversible decisions:** Just try it and iterate
- **Time-sensitive:** When delay costs more than error
- **Obvious no-brainers:** If all 6 steps are clearly yes, execute immediately

**When to Extend This Playbook:**
- **Existential decisions:** Add governance/ethics review
- **High-risk situations:** Add safety constraints before Step 6
- **Team decisions:** Add stakeholder alignment before commitment

---

### Success Metrics

**Leading Indicators (1-2 years):**
- Learning rate exceeding expectations
- Contrarian position being validated
- Exponential curve emerging
- Multiplicative wins compounding

**Lagging Indicators (5-10 years):**
- Decision created asymmetric returns
- Positioned ahead of competition
- Mission impact realized
- "Footnote" test passed

---

### Confidence & Limitations

**Playbook Confidence:** 91%

**High Confidence Steps:**
- Temporal Zoom-Out (98%)
- Exponential Detection (95%)
- Contrarian Check (90%)
- Commit with Speed (92%)

**Medium Confidence Steps:**
- Truth Stress-Test (85% - process clear, execution varies)
- Multiplicative Synthesis (87% - concept clear, application varies)

**Limitations:**
- Requires high tolerance for uncertainty
- Assumes resources available for execution
- Optimized for transformative decisions, not incremental
- May miss consensus opportunities that still work

---

## Crisis Response Playbook

**Use Case:** Handling extreme pressure, public criticism, organizational chaos, personal attacks

**Objective:** Maintain long-term mission focus and rational decision-making despite acute stress

---

### Input Criteria

This playbook applies when:
- Severe public criticism or controversy
- Organizational crisis (board drama, mass resignations, etc.)
- Personal reputation attacks
- High-stakes pressure requiring immediate response
- Emotional flooding threatening judgment

**Examples:** Board firing (Nov 2023), GPT release controversies, competitive threats, media attacks

---

### Step-by-Step Process

#### STEP 1: Temporal Reframe (Immediate - 2 minutes)

**Action:** Force perspective shift from immediate to long-term view

**Mental Exercise:**
```
Current state: [Acute crisis]
          ↓
    Zoom out to 10 years
          ↓
Question: "Will this matter in 10 years?"
          ↓
    Zoom out to 100 years
          ↓
Question: "What's the mission?"
```

**Key Phrases:**
- "The days are long but the decades are short"
- "This is like [tiny moment] between 2015 and AGI"
- "Most important work I'll ever touch"

**Outcome:**
- Emotional charge reduced
- Mission clarity restored
- Crisis sized appropriately

**Example (Board firing, Nov 2023):**
- Immediate: "I've been fired, this is chaos"
- 10-year view: "This was always going to happen sometime between 2015 and AGI"
- 100-year view: "AGI governance is the mission; this is learning"
- → Crisis reframed as iteration on governance

**Evidence:** "I could just feel like a victim forever or I can say this is like the most important work I'll ever touch in my life and I need to get back to it" (Lex Fridman 419)

---

#### STEP 2: Filter Status from Substance (5 minutes)

**Action:** Separate reputation/ego concerns from mission impact

**Questions:**
1. Is this about status (how I look) or substance (mission progress)?
2. Will this impact the actual work or just perception?
3. Am I defending my ego or the mission?
4. If I had no ego, what would I do?

**Decision Matrix:**

| Type | Response |
|------|----------|
| Pure status attack (no substance) | Ignore completely |
| Substance criticism (valid points) | Engage, learn, update |
| Mixed (status + substance) | Extract substance, ignore status |
| Mission-threatening | Address immediately |

**Red Flags:**
- Spending mental energy on "what people think"
- Crafting responses to protect image
- Avoiding critics instead of engaging substance
- Prioritizing vindication over mission

**Example (Media criticism):**
- Status concern: "Clickbait journalism bothers you"
- Sam's response: "Of all the things I lose sleep over it's not high on the list"
- → Pure status attack, ignore; focus on mission

**Evidence:** "Don't chase status. Status without substance doesn't work for long and is unfulfilling" (Days Are Long)

---

#### STEP 3: Truth-Seeking Under Pressure (30 minutes)

**Action:** Force intellectual honesty despite desire for self-protection

**Process:**

1. **Assume criticism is true:** What if the critics are right?
2. **Extract valid points:** What's the kernel of truth here?
3. **Check for self-delusion:** Am I avoiding reality?
4. **Seek disconfirming evidence:** What data contradicts my position?
5. **Update if necessary:** Change course if truth requires it

**Mindset:**
- "I used to hate criticism...now I try to always listen to it with the assumption that it's true"
- Separate critic's tone from content
- Strong opinions, weakly held

**Outcome Options:**
- **Update beliefs:** Critics identified real flaw → change course
- **Hold position:** Criticism examined and found lacking → maintain conviction
- **Partial update:** Mix of valid + invalid → selective adjustment

**Example (AI safety criticism):**
- Critics: "Moving too fast, deployment is reckless"
- Truth-seeking: Is there valid concern about safety?
- Extraction: Short timelines + iteration = safer than slow + one-shot
- Update: Strengthen safety protocols, maintain deployment strategy
- → Partial update, increased conviction in iterative approach

**Evidence:** "Truth-seeking is hard and often painful, but it is what separates self-belief from self-delusion" (How to Be Successful)

---

#### STEP 4: Decisive Action (Fast)

**Action:** Once truth determined, act immediately without second-guessing

**Decision Speed:**
- **Reversible decisions:** Immediate (hours)
- **Irreversible decisions:** Fast but deliberate (days, not weeks)
- **Avoid:** Extended deliberation, paralysis by analysis, consensus-seeking

**Implementation:**
1. **Clarity on action:** What needs to happen?
2. **Communication plan:** Transparent or private? Why?
3. **Execute:** Move with conviction
4. **No looking back:** Decision made, focus on execution

**Example (Returning to OpenAI):**
- Decision: Return as CEO
- Timeline: ~5 days from firing to return
- Communication: Public, transparent about governance learnings
- Execution: Immediate return to work
- → Fast, decisive, no second-guessing

**Evidence:** "I have never, not once, seen a slow-moving founder be really successful" (Startup Playbook)

---

#### STEP 5: Extract Learnings (After crisis resolved)

**Action:** Turn crisis into iteration for future improvement

**Questions:**
1. What did we learn about our systems/processes?
2. What warning signs did we miss?
3. How do we prevent this category of problem?
4. What made us antifragile vs fragile?

**Documentation:**
- Write down learnings while fresh
- Update playbooks/protocols
- Share with team (transparent)
- Incorporate into future strategy

**Example (Board drama learnings):**
- Learning: Governance structure had failure modes
- Warning signs: Misalignment between board and mission
- Prevention: Better board composition, clearer governance protocols
- Antifragility: Crisis strengthened organizational resilience
- → Emerged stronger with better governance

**Evidence:** "The only way I know how to solve a problem like this is iterating our way through it learning early and limiting the number of one shot to get it right scenarios" (Lex Fridman 367)

---

### Output & Follow-up

**Successful Crisis Response Results In:**
- Long-term perspective maintained
- Status concerns filtered out
- Truth-seeking executed despite pressure
- Decisive action taken quickly
- Learnings extracted for future improvement
- Mission continuity preserved

**Follow-up Actions:**
- Resume normal operations immediately
- Don't dwell on crisis (forward-looking)
- Monitor for similar patterns
- Strengthen revealed weaknesses

---

### Anti-Patterns to Avoid

**DON'T:**
- Play victim ("feel like a victim forever")
- Defend ego over mission
- Slow down decision-making due to pressure
- Seek consensus/validation when truth is clear
- Let critics dictate your agenda
- Lose sight of long-term mission

**DO:**
- Reframe crisis as learning opportunity
- Focus on substance over status
- Speed up truth-seeking and decisive action
- Update beliefs if warranted
- Maintain mission focus
- Extract maximum learning value

---

### Success Metrics

**Immediate (Days):**
- Emotional equilibrium restored
- Clear decision made and communicated
- Execution initiated

**Short-term (Weeks):**
- Crisis resolved or managed
- Team stability maintained
- Mission progress continued

**Long-term (Months-Years):**
- Crisis learnings integrated
- Antifragility increased
- No lasting mission damage
- Potentially stronger position than pre-crisis

---

### Confidence & Limitations

**Playbook Confidence:** 88%

**High Confidence Steps:**
- Temporal Reframe (95% - repeatedly demonstrated)
- Filter Status from Substance (92% - core value)
- Decisive Action (90% - consistent pattern)

**Medium Confidence Steps:**
- Truth-Seeking Under Pressure (85% - harder to execute under stress)
- Extract Learnings (85% - inferred from iterative philosophy)

**Limitations:**
- Requires extremely high stress tolerance
- Assumes mission clarity exists
- May not work for non-mission-driven individuals
- Optimized for reputational crises, less for existential threats

---

## Product Launch Playbook

**Use Case:** Launching new products, features, or models to users

**Objective:** Deploy iteratively to maximize learning while managing risk

---

### Input Criteria

This playbook applies when:
- New product/feature ready for users
- Significant uncertainty about user behavior
- Potential for unexpected consequences
- Need to balance capability with safety
- Opportunity for rapid iteration based on feedback

**Examples:** GPT model releases, ChatGPT launch, API releases, new features

---

### Step-by-Step Process

#### STEP 1: Define Limited Deployment Scope (Planning)

**Action:** Design initial deployment with safety constraints

**Questions:**
1. What's the smallest viable deployment to learn from?
2. What constraints ensure safe initial launch?
3. How do we measure what matters?
4. What's our expansion criteria?

**Scope Dimensions:**
- **User count:** How many users initially?
- **Capabilities:** Which features enabled/disabled?
- **Use cases:** What applications allowed?
- **Support:** How much human oversight?

**Safety Constraints:**
- Rate limits
- Content filters
- Human review loops
- Kill switches
- Monitoring systems

**Example (GPT-3 to GPT-3.5 to ChatGPT progression):**
- GPT-3: API only, limited access, researchers first
- GPT-3.5: Broader API, more use cases
- ChatGPT: Public web interface, free tier
- → Progressive expansion based on learnings

**Evidence:** "We want to make our mistakes while the stakes are low" (Lex Fridman 419)

---

#### STEP 2: Deploy & Measure Obsessively (Launch)

**Action:** Release and immediately instrument everything

**Implementation:**

1. **Launch:** Execute deployment to limited audience
2. **Announce:** Transparent communication about scope and limitations
3. **Instrument:** Log everything (usage patterns, failure modes, user feedback)
4. **Monitor:** Real-time dashboards for key metrics
5. **Support:** Human oversight for edge cases

**Key Metrics (Customize per product):**
- **User Value:** Task completion rate, user satisfaction, return rate
- **Safety:** Harmful outputs, abuse attempts, failure modes
- **Performance:** Speed, reliability, cost per query
- **Behavior:** What are users actually doing (vs what we expected)?

**Monitoring Intensity:**
- **Week 1:** Minute-by-minute monitoring
- **Week 2-4:** Hourly checks
- **Month 2+:** Daily reviews with weekly deep dives

**Example (ChatGPT launch):**
- Launched with basic web interface
- Measured: Usage patterns, successful vs failed queries, user feedback
- Discovered: Users wanted conversation, not just Q&A
- → Learned from reality, not assumptions

**Evidence:** "The company does what the CEO measures...it's valuable to have a single metric that the company optimizes" (Startup Playbook)

---

#### STEP 3: Learn from Reality (Days to Weeks)

**Action:** Let user behavior teach you what theory couldn't

**Questions:**
1. What are users actually doing (vs what we predicted)?
2. What's working unexpectedly well?
3. What's failing unexpectedly?
4. What use cases did we not anticipate?
5. What safety issues emerged?

**Analysis Process:**

```
Prediction → Reality → Gap Analysis → Update Model

For each surprise:
- Why were we wrong?
- What does this teach us about users/system?
- How should we update our model?
```

**Red Flags:**
- Confirmation bias (only seeing what you expected)
- Ignoring negative feedback
- Theory over empirical data
- Not talking to actual users

**Evidence Collection:**
- User interviews
- Usage logs
- Support tickets
- Social media feedback
- A/B test results

**Example (Iterative deployment discovery):**
- Prediction: Experts can predict AI behavior
- Reality: "Collective intelligence...helps us discover things we cannot imagine"
- Gap: User creativity exceeds expert predictions
- Update: Deploy broader to accelerate learning
- → Iterative deployment becomes strategy, not just tactic

**Evidence:** "The collective intelligence and ability of the outside world helps us discover things we cannot imagine" (Lex Fridman 419)

---

#### STEP 4: Update & Iterate (Continuous)

**Action:** Rapidly incorporate learnings into product improvements

**Iteration Cycle:**

```
Week 1-2: Learn from initial deployment
Week 3-4: Implement improvements
Week 5: Deploy improved version
Repeat
```

**Prioritization:**
1. **Safety issues:** Fix immediately
2. **High-value quick wins:** Ship within days
3. **Major improvements:** Plan for next version
4. **Long-term bets:** Add to roadmap

**Communication:**
- Transparent about changes
- Explain what you learned
- Acknowledge mistakes
- Show responsiveness to feedback

**Example (GPT model progression):**
- GPT-1 → GPT-2 → GPT-3 → GPT-3.5 → GPT-4
- Each version: Deploy, learn, improve, deploy next
- Not: Build GPT-5 in secret until perfect
- → "Rather than go build in secret until we got all the way to GPT 5 we decided to talk about GPT 1 2 3 and 4"

**Evidence:** "We think iterative deployment...rather than go build in secret until we got all the way to GPT 5 we decided to talk about GPT 1 2 3 and 4" (Lex Fridman 419)

---

#### STEP 5: Expand Thoughtfully (Based on Evidence)

**Action:** Increase scope only after safe expansion criteria met

**Expansion Criteria (Must meet ALL):**
- ✅ Safety metrics within acceptable bounds
- ✅ User value demonstrated clearly
- ✅ Major bugs/issues resolved
- ✅ Support load manageable
- ✅ Monitoring systems validated

**Expansion Dimensions:**
- More users (10x at a time)
- More capabilities (progressively unlock features)
- More use cases (broader applications)
- Less oversight (reduce human review)

**Pacing:**
- **Aggressive:** Double users weekly if all criteria met
- **Moderate:** 50% increase biweekly if some caution needed
- **Conservative:** Pause expansion if safety concerns emerge

**Example (API to ChatGPT expansion):**
- Started: Researchers only
- Expanded: API customers
- Further: Free web interface
- Each expansion: After safety and value validation
- → Progressive, evidence-based expansion

**Evidence:** "The only way I know how to solve a problem like this is iterating our way through it learning early and limiting the number of one shot to get it right scenarios" (Lex Fridman 367)

---

### Output & Follow-up

**Successful Product Launch Results In:**
- Safe initial deployment with constraints
- Rich empirical data on user behavior
- Rapid learning exceeding theoretical predictions
- Continuous improvement based on reality
- Evidence-based expansion to broader audience
- User trust through transparency

**Follow-up Actions:**
- Continue iteration cycles indefinitely
- Build organizational muscle for rapid deployment
- Document learnings for next product
- Scale monitoring/support infrastructure

---

### Success Metrics

**Launch Success (Week 1):**
- Deployment executed within scope
- No major safety incidents
- Monitoring systems functional
- User feedback flowing

**Learning Success (Month 1):**
- Surprises discovered and documented
- Mental model updated based on reality
- Clear improvements identified
- Iteration velocity high

**Scale Success (Quarter 1):**
- User count expanded significantly
- Safety maintained at scale
- Product-market fit demonstrated
- Competitive positioning achieved

---

### Confidence & Limitations

**Playbook Confidence:** 90%

**High Confidence Steps:**
- Deploy & Measure (95%)
- Learn from Reality (92%)
- Update & Iterate (93%)

**Medium Confidence Steps:**
- Define Limited Scope (85% - context-dependent)
- Expand Thoughtfully (87% - criteria inferred)

**Limitations:**
- Requires infrastructure for monitoring at scale
- May not work for irreversible deployments
- Optimized for software, less applicable to hardware
- Assumes ability to rapidly iterate (organizational capability)

---

## Hiring & Team Building Playbook

**Use Case:** Hiring decisions, team composition, evaluating talent

**Objective:** Build high-performing teams optimized for exponential growth and mission impact

---

### Input Criteria

This playbook applies when:
- Hiring for any role (junior to senior)
- Evaluating existing team members
- Building new teams or organizations
- Succession planning
- Performance reviews

**Examples:** Engineering hires, leadership roles, board composition, founding teams

---

### Step-by-Step Process

#### STEP 1: Define Role Type (Pre-screening)

**Action:** Categorize role to determine evaluation criteria

**Role Categories:**

**A. High-Slope Roles (Optimize for trajectory)**
- Individual contributors (engineers, researchers, designers)
- Junior to mid-level positions
- Roles where learning matters more than experience
- Fast-moving domains with rapid skill evolution

**B. High-Y-Intercept Roles (Optimize for track record)**
- Board members
- Advisors
- Executive leadership (context-dependent)
- Governance roles
- Domains where wisdom/experience critical

**C. Force-of-Nature Roles (Optimize for will + ability)**
- Founders (internal or external)
- Transformational leaders
- Roles requiring bending reality
- Mission-critical positions

**Decision Matrix:**

| Role Type | Primary Criterion | Secondary Criterion |
|-----------|------------------|-------------------|
| High-Slope | Rate of improvement | Current capability |
| High-Y-Intercept | Track record | Wisdom/judgment |
| Force-of-Nature | Will to make things happen | Slope or Y-intercept |

**Evidence:** "There's some roles where I kind of totally ignore track record and just look at slope...for a board member like I do care much more about the Y intercept" (Lex Fridman 419)

---

#### STEP 2: Evaluate Slope (For High-Slope Roles)

**Action:** Assess rate of improvement, not current state

**Questions:**
1. How fast has this person improved in past 1-2 years?
2. What's their learning velocity on new challenges?
3. Are they on exponential trajectory or plateau?
4. How do they respond to feedback?

**Assessment Methods:**

**A. Historical trajectory:**
- Map skills/achievements over time
- Calculate improvement rate
- Look for inflection points (acceleration)

**B. Learning mindset:**
- Ask: "Tell me about something you were bad at and got good at"
- Look for: Speed of improvement, methods used, self-awareness
- Red flags: Fixed mindset, blame others, no self-reflection

**C. Trajectory projection:**
- If they improve at this rate, where are they in 2 years?
- Do I want that person on my team?
- Is this slope sustainable or temporary?

**Scoring:**
- **10/10 Slope:** Exponential improvement, insatiable learning, 2x better every 6 months
- **7-9/10 Slope:** Strong improvement, consistent learning, 50%+ better per year
- **4-6/10 Slope:** Moderate improvement, some learning, linear growth
- **<4/10 Slope:** Plateau, declining, or negative trajectory

**Decision Criteria:**
- ✅ **HIRE** if slope >8/10 (even if current capability is moderate)
- ⚠️ **MAYBE** if slope 6-8/10 (depends on role urgency)
- ❌ **PASS** if slope <6/10 (unlikely to become great)

**Example (Evaluating junior engineer):**
- Current capability: 6/10 (decent but not exceptional)
- Slope: Started at 2/10 two years ago, now 6/10
- Trajectory: 3x improvement in 2 years = ~75% annual growth
- Projection: At this rate, 9/10 in 2 more years
- → ✅ HIRE (slope matters more than current state)

---

#### STEP 3: Evaluate Y-Intercept (For High-Y-Intercept Roles)

**Action:** Assess track record, wisdom, and judgment

**Questions:**
1. What's their track record in this domain?
2. Have they navigated similar challenges before?
3. Do they have pattern-matching wisdom?
4. How's their judgment under pressure?

**Assessment Methods:**

**A. Track record analysis:**
- What have they actually accomplished?
- How did they perform in crisis situations?
- What's their reputation among peers?

**B. Judgment evaluation:**
- Present complex scenarios
- Evaluate: How they frame problems, what they prioritize, decision quality
- Look for: Wisdom, nuance, long-term thinking

**C. Governance capability:**
- Can they oversee without micromanaging?
- Do they provide strategic guidance vs tactical?
- Are they ego-driven or mission-driven?

**Scoring:**
- **10/10 Y-Intercept:** Exceptional track record, proven judgment, widely respected
- **7-9/10:** Strong track record, good judgment, known in field
- **4-6/10:** Moderate experience, developing judgment
- **<4/10:** Limited track record or poor judgment

**Decision Criteria:**
- ✅ **HIRE** if Y-intercept >8/10 (experience/wisdom critical)
- ⚠️ **MAYBE** if Y-intercept 6-8/10 (depends on role requirements)
- ❌ **PASS** if Y-intercept <6/10 (insufficient experience for role)

**Example (Evaluating board member):**
- Track record: Built multiple successful companies, navigated crises
- Judgment: Proven decision-making under pressure
- Reputation: Widely respected in industry
- Y-Intercept: 9/10
- → ✅ HIRE (track record and wisdom matter most)

**Evidence:** "For a board member like I do care much more about the Y intercept" (Lex Fridman 419)

---

#### STEP 4: Evaluate Force-of-Nature (For Transformational Roles)

**Action:** Assess will, drive, and reality-bending capability

**Core Question:** "Does this person make things happen in the world?"

**Sub-Questions:**
1. Are they internally or externally motivated?
2. Do they bend reality to their will?
3. Have they accomplished unlikely things?
4. Do they inspire others to extraordinary effort?
5. Are they driven by mission or status?

**Assessment Methods:**

**A. Internal vs External Drive:**
- Ask: "Why do you do what you do?"
- Look for: Internal motivation, mission-driven, indifferent to status
- Red flags: Status-seeking, external validation needs, impressing others

**B. Reality-Bending Evidence:**
- Have they accomplished things people said were impossible?
- Do obstacles slow them or energize them?
- How do they respond to "no"?

**C. Mission Alignment:**
- Do they care about the mission or just the role?
- Would they do this even if no one was watching?
- Are they in it for the long-term?

**Scoring:**
- **Force-of-Nature:** Internally driven, accomplishes impossible things, bends reality consistently
- **Strong Executor:** Gets things done but needs external structure
- **Capable But Needs Direction:** Competent but not self-driven
- **Status-Seeker:** Externally motivated, unlikely to sustain through difficulty

**Decision Criteria:**
- ✅ **HIRE** if genuine Force-of-Nature detected (rare, treasure when found)
- ⚠️ **MAYBE** if Strong Executor (depends on role needs)
- ❌ **PASS** if Status-Seeker (misalignment with mission-driven culture)

**Example (Evaluating founding team member):**
- Internal motivation: 10/10 (doesn't care what others think)
- Reality-bending: 9/10 (has accomplished unlikely things repeatedly)
- Mission alignment: 10/10 (would do this regardless of recognition)
- Assessment: Clear Force-of-Nature
- → ✅ HIRE immediately (rare find)

**Evidence:** "The most successful people I know are primarily internally driven; they do what they do to impress themselves and because they feel compelled to make something happen in the world" (How to Be Successful)

---

#### STEP 5: Speed of Decision (Execute Fast)

**Action:** Decide quickly once evaluation complete

**Decision Speed Guidelines:**

**Slam-Dunk Candidates:**
- Decision: Same day
- Action: Make offer immediately
- Why: Great people have options; speed signals conviction

**Strong Candidates:**
- Decision: 1-3 days
- Action: Quick offer with clear enthusiasm
- Why: Don't lose good people to slow process

**Uncertain Candidates:**
- Decision: 1 week max
- Action: Either hire or pass; no limbo
- Why: Uncertainty doesn't improve with time

**Red Flag Candidates:**
- Decision: Immediate pass
- Action: Respectful decline, fast
- Why: No need to delay bad news

**Anti-Pattern:** Slow, consensus-driven hiring processes that lose great candidates

**Evidence:** "I have never, not once, seen a slow-moving founder be really successful" (Startup Playbook)

---

### Special Cases

#### A. Evaluating Existing Team Members

**Process:**
1. **Slope check:** Are they still improving or plateauing?
2. **Mission alignment:** Still driven or going through motions?
3. **Force-of-nature:** Still bending reality or coasting?
4. **Decision:** If declining slope or misaligned → act fast

**Common Mistake:** Keeping people too long after slope flattens

---

#### B. Building Teams (Not Just Individuals)

**Team Composition Principles:**

1. **Multiplicative Advantage:** Many good people > few superstars
2. **Diversity of Slopes:** Mix high-slope juniors with high-Y-intercept seniors
3. **Shared Values:** Internal drive, truth-seeking, mission-first
4. **Complementary Skills:** Cover all necessary capabilities

**Team Red Flags:**
- All Y-intercept (wisdom) but no slope (energy)
- All slope (energy) but no Y-intercept (judgment)
- Status-seekers infiltrating culture
- Lack of truth-seeking mindset

---

### Output & Follow-up

**Successful Hiring Results In:**
- Right people in right roles (slope vs Y-intercept matched)
- Fast hiring process (don't lose great candidates)
- Mission-aligned, internally-driven team
- Force-of-nature people in transformational roles
- Culture of truth-seeking and exponential improvement

**Follow-up Actions:**
- Onboard with clear mission alignment
- Measure slope continuously
- Provide feedback for improvement
- Remove misaligned people quickly

---

### Success Metrics

**Immediate (Hire Decision):**
- Right criteria used for role type
- Decision made quickly
- Offer accepted (great candidates say yes)

**Short-term (3-6 months):**
- New hire performing at expected slope
- Cultural fit validated
- Mission alignment demonstrated

**Long-term (1-2 years):**
- High-slope hires becoming top performers
- High-Y-intercept hires providing wisdom/guidance
- Force-of-nature hires bending reality
- Team compounding capabilities

---

### Confidence & Limitations

**Playbook Confidence:** 87%

**High Confidence Steps:**
- Define Role Type (92%)
- Evaluate Slope (90%)
- Evaluate Force-of-Nature (88%)
- Speed of Decision (92%)

**Medium Confidence Steps:**
- Evaluate Y-Intercept (85% - judgment assessment is harder)

**Limitations:**
- Slope evaluation requires historical data (harder for career changers)
- Force-of-nature detection can be fooled by charisma
- Cultural fit assessment not fully captured here
- Optimized for mission-driven organizations

---

## Meta-Playbook: When to Use Which Playbook

| Situation | Playbook | Why |
|-----------|----------|-----|
| Career decision, market entry, founding company | Strategic Decision | Long-term, high-stakes, transformative |
| Public criticism, board drama, reputational crisis | Crisis Response | Maintain mission focus under pressure |
| Shipping product, launching feature, model release | Product Launch | Maximize learning, manage risk |
| Hiring, team building, performance review | Hiring & Team Building | Build high-performing teams |

---

## Universal Principles Across All Playbooks

1. **Long-term thinking:** Zoom out to 10+ years
2. **Exponential optimization:** Seek compound growth everywhere
3. **Truth-seeking:** Reality over self-delusion
4. **Speed of execution:** Decide fast, move fast
5. **Iterative learning:** Deploy, measure, learn, improve
6. **Mission over ego:** Substance over status
7. **Internal drive:** Don't rely on external validation

---

**Document Status:** Production Ready
**Next Phase:** QA Dataset (testing these playbooks with real scenarios)
**Revision Date:** 2025-10-11
**Source Confidence:** 87-91% across playbooks
