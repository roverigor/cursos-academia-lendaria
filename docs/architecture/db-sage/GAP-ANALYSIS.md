# DB Sage - An√°lise Completa de Gaps & Best Practices

**Data**: 2025-10-27
**Vers√£o**: 1.0.0
**An√°lise por**: Alan (Database Expert) + Winston (Architect)
**Status**: ‚ö†Ô∏è Gaps Cr√≠ticos Identificados

---

## üìä Executive Summary

**Status Geral**: ‚úÖ Fundamento S√≥lido | ‚ö†Ô∏è Gaps Cr√≠ticos Impedem Produ√ß√£o

**Score Global: 6/10** - N√£o est√° production-ready sem corre√ß√µes dos gaps cr√≠ticos.

### Pontos Fortes
- ‚úÖ Filosofia safety-first bem implementada (snapshots, dry-run, rollback)
- ‚úÖ Estrutura operacional DBA s√≥lida
- ‚úÖ Templates de documenta√ß√£o abrangentes
- ‚úÖ Princ√≠pios core bem definidos

### Gaps que Bloqueiam Produ√ß√£o

| # | Gap | Severidade | Impacto |
|---|-----|------------|---------|
| 1 | Schema Version Tracking ausente | üî¥ CR√çTICO | Imposs√≠vel rastrear migrations |
| 2 | Zero-Downtime Migrations n√£o implementado | üî¥ CR√çTICO | Downtime obrigat√≥rio |
| 3 | Backup/Disaster Recovery ausente | üî¥ CR√çTICO | Risco de perda de dados |
| 4 | Monitoring/Observability n√£o integrado | üî¥ CR√çTICO | Sem visibilidade operacional |
| 5 | RLS Patterns incompletos | üü° ALTO | Seguran√ßa inconsistente |
| 6 | Connection Pooling n√£o detalhado | üü° ALTO | Performance issues |

**Recomenda√ß√£o**: Implementar os 4 gaps cr√≠ticos antes de usar em produ√ß√£o.

---

## 1. PostgreSQL Best Practices

### ‚úÖ O Que Est√° Correto

```yaml
Princ√≠pios Implementados:
  - Constraints no DB (n√£o na app) ‚úÖ
  - Foreign keys obrigat√≥rias ‚úÖ
  - Indexes baseados em access patterns ‚úÖ
  - Transa√ß√µes expl√≠citas ‚úÖ
  - EXPLAIN analysis workflow ‚úÖ
  - Idempotency nos scripts ‚úÖ

Design Patterns Presentes:
  - updated_at triggers ‚úÖ
  - Soft deletes mencionados ‚úÖ
  - Primary keys padronizados ‚úÖ
  - COMMENT ON para documenta√ß√£o ‚úÖ
```

### ‚ùå GAP 1.1: Schema Version Tracking

**Severidade**: üî¥ CR√çTICO

**Problema**:
- N√£o existe tabela `schema_migrations` customizada
- Supabase migrations n√£o trackam quem aplicou, checksums, rollback scripts
- Imposs√≠vel validar integridade de migrations

**Solu√ß√£o Necess√°ria**:
```sql
-- ADICIONAR em templates e tasks

CREATE TABLE IF NOT EXISTS public.schema_migrations (
  version TEXT PRIMARY KEY,
  name TEXT NOT NULL,
  applied_at TIMESTAMPTZ DEFAULT NOW(),
  applied_by TEXT NOT NULL,
  execution_time_ms INTEGER,
  success BOOLEAN NOT NULL,
  checksum TEXT NOT NULL,        -- SHA256 do arquivo
  rollback_script TEXT,           -- Script de rollback
  notes TEXT,
  CONSTRAINT valid_checksum CHECK (length(checksum) = 64)
);

CREATE INDEX idx_migrations_applied_at
  ON schema_migrations(applied_at DESC);

COMMENT ON TABLE schema_migrations IS
  'Custom migration tracking with checksums and rollback scripts';
```

**Tasks Afetadas**:
- `db-apply-migration.md` - adicionar inser√ß√£o em schema_migrations
- `db-verify-order.md` - validar contra schema_migrations
- `db-rollback.md` - usar rollback_script da tabela

**Templates Afetados**:
- `migration-plan-tmpl.yaml` - adicionar se√ß√£o de version tracking

---

### ‚ùå GAP 1.2: Connection Pooling Strategy

**Severidade**: üü° ALTO

**Problema**:
- Menciona pooler (6543) mas sem detalhes
- Transaction vs Session mode n√£o documentado
- Pool sizing n√£o explicado
- Quando usar pooler vs direct n√£o claro

**Solu√ß√£o Necess√°ria**:

```yaml
# ADICIONAR em data/postgres-connection-pooling.md

connection_pooling:
  supabase_pooler:
    port: 6543
    mode: transaction         # vs session
    use_when:
      - serverless_functions
      - high_concurrency
      - short_lived_connections

  direct_connection:
    port: 5432
    mode: session
    use_when:
      - migrations
      - long_transactions
      - advisory_locks
      - listen_notify

  pool_sizing:
    formula: "(CPU_cores * 2) + effective_spindle_count"
    example:
      cpu_cores: 4
      spindles: 1
      recommended_pool_size: 9
      max_connections: 100

  configuration:
    transaction_mode:
      pros:
        - higher_concurrency
        - connection_reuse
      cons:
        - no_prepared_statements
        - no_advisory_locks
    session_mode:
      pros:
        - full_postgres_features
        - prepared_statements
      cons:
        - lower_concurrency
```

**Data Files Necess√°rios**:
- ‚úÖ Criar `data/postgres-connection-pooling.md`
- ‚úÖ Documentar quando usar cada modo
- ‚úÖ Exemplos de configura√ß√£o

---

### ‚ùå GAP 1.3: Vacuum & Maintenance

**Severidade**: üü° ALTO

**Problema**:
- Sem estrat√©gia de VACUUM ANALYZE
- Sem monitoramento de bloat
- Sem cleanup de dead tuples
- Sem queries de pg_stat_*

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR task: db-maintenance-check.md

-- 1. Dead Tuples Check
SELECT
  schemaname,
  relname,
  n_live_tup,
  n_dead_tup,
  ROUND(n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0), 2) AS dead_pct,
  last_vacuum,
  last_autovacuum
FROM pg_stat_user_tables
WHERE n_dead_tup > 1000
ORDER BY dead_pct DESC
LIMIT 20;

-- 2. Table Bloat Check
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) -
                 pg_relation_size(schemaname||'.'||tablename)) AS bloat
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 10;

-- 3. Index Bloat Check
SELECT
  schemaname,
  tablename,
  indexname,
  pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
  idx_scan,
  idx_tup_read,
  idx_tup_fetch
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY pg_relation_size(indexrelid) DESC
LIMIT 10;
```

**Tasks Necess√°rias**:
- ‚úÖ Criar `db-maintenance-check.md`
- ‚úÖ Criar `db-vacuum-analyze.md`
- ‚úÖ Adicionar a checklist de monitoring

---

### ‚ùå GAP 1.4: Indexing Strategy Incompleto

**Severidade**: üü¢ M√âDIO

**Problema**:
- Falta partial indexes (WHERE clauses)
- Falta expression indexes (LOWER(email))
- Falta covering indexes (INCLUDE)
- Falta GIN indexes para JSONB
- Falta GiST indexes

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em templates/index-strategy-tmpl.yaml

advanced_index_types:
  partial_indexes:
    description: "Index only subset of rows"
    use_when: "Queries always filter by same condition"
    example: |
      CREATE INDEX idx_active_users ON users(created_at)
      WHERE deleted_at IS NULL;

      -- Much smaller index, faster queries on active users

  expression_indexes:
    description: "Index on computed values"
    use_when: "Queries use functions on columns"
    example: |
      CREATE INDEX idx_users_email_lower ON users(LOWER(email));

      -- Now can use: WHERE LOWER(email) = 'user@example.com'

  covering_indexes:
    description: "Include extra columns in index"
    use_when: "Avoid table lookups (index-only scan)"
    example: |
      CREATE INDEX idx_users_email_covering
      ON users(email)
      INCLUDE (name, created_at);

      -- Query can be satisfied entirely from index

  gin_indexes:
    description: "For JSONB, arrays, full-text search"
    use_when: "Querying JSONB fields or arrays"
    example: |
      CREATE INDEX idx_metadata_gin ON documents
      USING GIN (metadata jsonb_path_ops);

      -- Fast: WHERE metadata @> '{"status": "published"}'

  gist_indexes:
    description: "For ranges, geometry, ltree"
    use_when: "Range queries or geometric data"
    example: |
      CREATE INDEX idx_events_timerange ON events
      USING GIST (timerange);

      -- Fast: WHERE timerange && '[2024-01-01, 2024-12-31]'
```

**Templates Afetados**:
- ‚ö†Ô∏è `index-strategy-tmpl.yaml` - expandir significativamente

---

### ‚ùå GAP 1.5: Query Optimization Patterns

**Severidade**: üü¢ M√âDIO

**Problema**:
- Falta guidance sobre CTEs vs subqueries
- Falta LATERAL joins patterns
- Falta window functions optimization
- Falta array operations optimization
- Falta JSONB query patterns

**Solu√ß√£o Necess√°ria**:

Criar `data/postgres-query-optimization.md` com:
- CTEs vs subqueries (quando usar cada)
- LATERAL joins (quando e como)
- Window functions (partitioning, ordering)
- Array operations (ANY, ALL, ARRAY_AGG)
- JSONB optimization (@>, ?, ->, ->>)

---

## 2. Supabase Best Practices

### ‚úÖ O Que Est√° Correto

```yaml
Integra√ß√£o B√°sica:
  - RLS como core principle ‚úÖ
  - auth.uid() usage ‚úÖ
  - Pooler awareness ‚úÖ
  - Realtime mencionado ‚úÖ
  - Edge Functions mencionados ‚úÖ

Security Basics:
  - Service role warnings ‚úÖ
  - SSL enforcement ‚úÖ
  - Secrets redaction ‚úÖ
```

### ‚ùå GAP 2.1: Realtime Configuration

**Severidade**: üü° ALTO

**Problema**:
- Sem configura√ß√£o de publications
- Sem monitoramento de replication slots
- Sem management de WAL size

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR task: db-realtime-setup.md

-- 1. Configure Publications
ALTER PUBLICATION supabase_realtime ADD TABLE users;
ALTER PUBLICATION supabase_realtime ADD TABLE posts;
ALTER PUBLICATION supabase_realtime
  SET (publish = 'insert, update, delete');

-- 2. Monitor Replication Slots
SELECT
  slot_name,
  slot_type,
  active,
  wal_status,
  pg_size_pretty(
    pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)
  ) AS replication_lag
FROM pg_replication_slots;

-- 3. Monitor WAL Size
SELECT
  pg_size_pretty(
    pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0')
  ) AS current_wal_size;

-- Alert if > 1GB
```

**Tasks Necess√°rias**:
- ‚úÖ Criar `db-realtime-setup.md`
- ‚úÖ Criar `db-realtime-monitor.md`

---

### ‚ùå GAP 2.2: Storage Integration

**Severidade**: üî¥ CR√çTICO (se usar Storage)

**Problema**:
- Supabase Storage + RLS n√£o documentado
- Policies para `storage.objects` ausentes
- Bucket security n√£o mencionada

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em templates/rls-policies-tmpl.yaml

storage_policies:
  description: "RLS policies for Supabase Storage buckets"

  upload_policy:
    example: |
      CREATE POLICY "Users upload own avatars"
      ON storage.objects FOR INSERT
      TO authenticated
      WITH CHECK (
        bucket_id = 'avatars' AND
        auth.uid()::text = (storage.foldername(name))[1]
      );

  read_policy:
    example: |
      CREATE POLICY "Public avatars readable"
      ON storage.objects FOR SELECT
      TO public
      USING (bucket_id = 'avatars');

  delete_policy:
    example: |
      CREATE POLICY "Users delete own files"
      ON storage.objects FOR DELETE
      TO authenticated
      USING (
        bucket_id = 'avatars' AND
        auth.uid()::text = (storage.foldername(name))[1]
      );
```

**Templates Afetados**:
- ‚ö†Ô∏è `rls-policies-tmpl.yaml` - adicionar se√ß√£o storage

---

### ‚ùå GAP 2.3: Edge Functions + Database

**Severidade**: üü° ALTO

**Problema**:
- Pattern para Edge Functions chamando DB n√£o documentado
- Connection pooling from Edge ausente
- JWT validation patterns n√£o mencionado

**Solu√ß√£o Necess√°ria**:

```typescript
// ADICIONAR em data/supabase-edge-functions.md

// 1. Edge Function ‚Üí Database Pattern
import { createClient } from '@supabase/supabase-js'

Deno.serve(async (req) => {
  // Get JWT from Authorization header
  const authHeader = req.headers.get('Authorization')!

  // Create client with user's JWT (RLS applies)
  const supabase = createClient(
    Deno.env.get('SUPABASE_URL')!,
    Deno.env.get('SUPABASE_ANON_KEY')!,
    {
      global: {
        headers: { Authorization: authHeader }
      },
      db: { schema: 'public' }
    }
  )

  // Query respects RLS
  const { data, error } = await supabase
    .from('users')
    .select('*')
    .eq('id', 'user-id')

  return new Response(JSON.stringify(data))
})

// 2. Service Role Pattern (bypasses RLS - dangerous!)
const supabaseAdmin = createClient(
  Deno.env.get('SUPABASE_URL')!,
  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!  // ‚ö†Ô∏è Use sparingly
)
```

**Data Files Necess√°rios**:
- ‚úÖ Criar `data/supabase-edge-functions.md`

---

### ‚ùå GAP 2.4: Auth Hooks

**Severidade**: üü¢ M√âDIO

**Problema**:
- Auth webhooks integration n√£o documentado
- Custom claims patterns ausente
- MFA considerations n√£o mencionado

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em data/supabase-auth-patterns.md

-- 1. New User Trigger
CREATE OR REPLACE FUNCTION handle_new_user()
RETURNS TRIGGER AS $$
BEGIN
  INSERT INTO public.profiles (id, email, role)
  VALUES (NEW.id, NEW.email, 'user');
  RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

CREATE TRIGGER on_auth_user_created
  AFTER INSERT ON auth.users
  FOR EACH ROW
  EXECUTE FUNCTION handle_new_user();

-- 2. Custom Claims (JWT)
CREATE OR REPLACE FUNCTION custom_access_token_hook(event jsonb)
RETURNS jsonb AS $$
DECLARE
  claims jsonb;
  user_role text;
BEGIN
  -- Get user role
  SELECT role INTO user_role
  FROM public.profiles
  WHERE id = (event->>'user_id')::uuid;

  -- Add to JWT claims
  claims := event->'claims';
  claims := jsonb_set(claims, '{user_role}', to_jsonb(user_role));

  RETURN jsonb_set(event, '{claims}', claims);
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

---

### ‚ùå GAP 2.5: RLS Patterns Incompletos

**Severidade**: üî¥ CR√çTICO

**Problema**:
- Tem KISS policies, mas falta patterns avan√ßados:
  - Multi-tenancy
  - Time-based (scheduled content)
  - Hierarchical (org > team > user)
  - Role-based com custom claims

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em templates/rls-policies-tmpl.yaml

advanced_rls_patterns:

  multi_tenancy:
    description: "Tenant isolation usando JWT claims"
    example: |
      CREATE POLICY tenant_isolation ON documents
        FOR ALL TO authenticated
        USING (
          tenant_id = (auth.jwt() ->> 'tenant_id')::uuid
        );

      -- Garante que user s√≥ v√™ dados do seu tenant

  time_based:
    description: "Content scheduling (publish_at / expire_at)"
    example: |
      CREATE POLICY scheduled_content ON posts
        FOR SELECT TO authenticated
        USING (
          (publish_at IS NULL OR publish_at <= NOW()) AND
          (expire_at IS NULL OR expire_at > NOW())
        );

      -- Post s√≥ vis√≠vel entre publish_at e expire_at

  hierarchical:
    description: "Org > Team > User hierarchy"
    example: |
      CREATE POLICY org_hierarchy ON resources
        FOR SELECT TO authenticated
        USING (
          org_id IN (
            SELECT org_id
            FROM user_org_memberships
            WHERE user_id = auth.uid()
          )
        );

      -- User v√™ recursos de todas orgs que pertence

  role_based_claims:
    description: "Role-based usando JWT custom claims"
    example: |
      CREATE POLICY admin_full_access ON sensitive_data
        FOR ALL TO authenticated
        USING (
          (auth.jwt() ->> 'role') = 'admin'
        );

      WITH CHECK (
        (auth.jwt() ->> 'role') IN ('admin', 'manager')
      );

      -- Admin: read/write, Manager: write only, User: no access
```

**Templates Afetados**:
- ‚ö†Ô∏è `rls-policies-tmpl.yaml` - expandir MUITO

---

## 3. Opera√ß√µes & DBA

### ‚úÖ O Que Est√° Correto

```yaml
Workflows Implementados:
  - Snapshot before migration ‚úÖ
  - Dry-run testing ‚úÖ
  - Smoke tests ‚úÖ
  - Rollback procedures ‚úÖ
  - Idempotent operations ‚úÖ

Safety Mechanisms:
  - Transaction wrapping ‚úÖ
  - Error handling ‚úÖ
  - Secrets redaction ‚úÖ
```

### ‚ùå GAP 3.1: Zero-Downtime Migrations

**Severidade**: üî¥ CR√çTICO

**Problema**:
- Workflow atual n√£o suporta zero-downtime
- Expand/Contract pattern n√£o implementado
- Todas migrations causam downtime

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR task: db-zero-downtime-migration.md

-- FASE 1: EXPAND (additive only, app old version)
ALTER TABLE users ADD COLUMN new_email TEXT;
CREATE INDEX CONCURRENTLY idx_new_email ON users(new_email);

-- Deploy app version that writes to BOTH columns

-- FASE 2: BACKFILL (migrate data)
UPDATE users
SET new_email = old_email
WHERE new_email IS NULL;

-- FASE 3: VALIDATE (check data integrity)
SELECT count(*) FROM users WHERE new_email IS NULL;  -- Should be 0

-- Deploy app version that reads from new_email

-- FASE 4: CONTRACT (remove old, app new version)
ALTER TABLE users DROP COLUMN old_email;
```

**Tasks Necess√°rias**:
- ‚úÖ Criar `db-zero-downtime-migration.md`
- ‚úÖ Documentar pattern expand/contract
- ‚úÖ Exemplos para: add column, rename column, change type, split table

---

### ‚ùå GAP 3.2: Backup & Disaster Recovery

**Severidade**: üî¥ CR√çTICO

**Problema**:
- Backup strategy n√£o existe
- Point-in-time recovery n√£o implementado
- Cross-region replication n√£o mencionado
- Backup verification ausente

**Solu√ß√£o Necess√°ria**:

```bash
# ADICIONAR tasks:
# - db-backup.md
# - db-backup-verify.md
# - db-restore-pit.md

# 1. Full Backup
pg_dump "${SUPABASE_DB_URL}" \
  --format=custom \
  --compress=9 \
  --file="backup_$(date +%Y%m%d_%H%M%S).dump"

# 2. Schema-only Backup
pg_dump "${SUPABASE_DB_URL}" \
  --schema-only \
  --format=plain \
  --file="schema_$(date +%Y%m%d).sql"

# 3. Verify Backup
pg_restore --list backup.dump | head -20

# 4. Test Restore to temp DB
pg_restore --dbname="${TEST_DB_URL}" backup.dump

# 5. Point-in-Time Recovery (Supabase feature)
# Via Supabase Dashboard: Database > Backups > Restore to point
```

**Tasks Necess√°rias**:
- ‚úÖ Criar `db-backup.md`
- ‚úÖ Criar `db-backup-verify.md`
- ‚úÖ Criar `db-restore.md`
- ‚úÖ Criar `db-restore-pit.md` (Supabase PITR)

---

### ‚ùå GAP 3.3: Monitoring & Alerting

**Severidade**: üî¥ CR√çTICO

**Problema**:
- Sem integra√ß√£o com observability
- Sem metrics collection
- Sem alert thresholds
- Sem slow query log

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR task: db-monitoring-setup.md

-- 1. Enable pg_stat_statements
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- 2. Slow Query Monitor
SELECT
  calls,
  mean_exec_time,
  max_exec_time,
  stddev_exec_time,
  query
FROM pg_stat_statements
WHERE mean_exec_time > 1000  -- > 1 second
ORDER BY mean_exec_time DESC
LIMIT 20;

-- 3. Connection Pool Saturation
SELECT
  count(*) AS current_connections,
  max_connections,
  ROUND(count(*)::numeric / max_connections::numeric * 100, 2) AS usage_pct
FROM pg_stat_activity,
     (SELECT setting::int AS max_connections FROM pg_settings WHERE name = 'max_connections') mc
GROUP BY max_connections;
-- Alert if usage_pct > 80%

-- 4. Replication Lag (if applicable)
SELECT
  client_addr,
  state,
  sync_state,
  pg_wal_lsn_diff(pg_current_wal_lsn(), sent_lsn) AS lag_bytes,
  pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), sent_lsn)) AS lag_pretty
FROM pg_stat_replication;
-- Alert if lag_bytes > 10MB

-- 5. Table Bloat Alert
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
  n_dead_tup,
  ROUND(n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0), 2) AS dead_pct
FROM pg_stat_user_tables
WHERE n_dead_tup > 10000
  AND n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0) > 20
ORDER BY dead_pct DESC;
-- Alert if dead_pct > 20%
```

**Tasks Necess√°rias**:
- ‚úÖ Criar `db-monitoring-setup.md`
- ‚úÖ Criar `db-monitor-queries.md` (j√° existe, melhorar)
- ‚úÖ Criar `db-check-locks.md` (j√° existe, melhorar)
- ‚úÖ Criar `db-alert-thresholds.md`

---

### ‚ùå GAP 3.4: Migration History Tracking

**Severidade**: üî¥ CR√çTICO (relacionado a GAP 1.1)

**Problema**:
- Sem checksum validation
- Sem rollback script association
- Sem tracking de quem aplicou

**Solu√ß√£o**: Ver GAP 1.1 (Schema Version Tracking)

---

### ‚ùå GAP 3.5: Health Checks B√°sicos Demais

**Severidade**: üü° ALTO

**Problema**:
- Smoke tests existem mas s√£o superficiais
- Falta dependency graph validation
- Falta orphaned indexes check
- Falta missing FK indexes check

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em db-smoke-test.md (expand)

-- 1. Dependency Graph (detect cycles)
WITH RECURSIVE deps AS (
  SELECT
    conrelid::regclass AS table_name,
    confrelid::regclass AS referenced_table,
    conname AS constraint_name
  FROM pg_constraint
  WHERE contype = 'f'
)
SELECT * FROM deps;
-- Validate no cycles

-- 2. Orphaned Indexes (never used)
SELECT
  schemaname,
  tablename,
  indexname,
  idx_scan,
  pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexname NOT LIKE '%_pkey'
  AND pg_relation_size(indexrelid) > 1024*1024  -- > 1MB
ORDER BY pg_relation_size(indexrelid) DESC;

-- 3. Missing FK Indexes
SELECT
  tc.table_name,
  kcu.column_name,
  ccu.table_name AS foreign_table_name,
  ccu.column_name AS foreign_column_name
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND NOT EXISTS (
    SELECT 1
    FROM pg_indexes
    WHERE tablename = tc.table_name
      AND indexdef LIKE '%' || kcu.column_name || '%'
  );
-- These FKs should have indexes!
```

---

## 4. Arquitetura de Dados

### ‚úÖ O Que Est√° Correto

```yaml
Design Principles:
  - Domain-driven design ‚úÖ
  - Access pattern first ‚úÖ
  - Normalization pragm√°tica ‚úÖ
  - Defense in depth ‚úÖ

Schema Design:
  - Templates abrangentes ‚úÖ
  - Documentation embedded ‚úÖ
  - Constraints first ‚úÖ
```

### ‚ùå GAP 4.1: Partitioning

**Severidade**: üü¢ M√âDIO

**Problema**:
- N√£o menciona quando/como particionar
- Time-based partitioning ausente
- Partition pruning n√£o explicado

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em data/postgres-partitioning.md

-- Time-based Partitioning Example
CREATE TABLE events (
  id BIGSERIAL,
  user_id UUID NOT NULL,
  event_type TEXT NOT NULL,
  created_at TIMESTAMPTZ NOT NULL,
  data JSONB
) PARTITION BY RANGE (created_at);

-- Create partitions
CREATE TABLE events_2024_01 PARTITION OF events
  FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE events_2024_02 PARTITION OF events
  FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- Indexes on each partition
CREATE INDEX idx_events_2024_01_user ON events_2024_01(user_id);
CREATE INDEX idx_events_2024_02_user ON events_2024_02(user_id);

-- Auto-create partitions (using pg_cron or trigger)
-- Query benefits from partition pruning:
-- WHERE created_at >= '2024-01-15' AND created_at < '2024-01-20'
-- Only scans events_2024_01 partition
```

**Data Files Necess√°rios**:
- ‚úÖ Criar `data/postgres-partitioning.md`

---

### ‚ùå GAP 4.2: JSONB Strategy

**Severidade**: üü¢ M√âDIO

**Problema**:
- JSONB patterns superficiais
- Sem guidance sobre JSONB vs separate tables
- Sem JSONB indexes (GIN)
- Sem JSONB constraints

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em data/postgres-jsonb-patterns.md

-- 1. JSONB vs Separate Tables Decision Tree
use_jsonb_when:
  - schema_is_variable
  - data_is_sparse
  - querying_full_object_mostly
  - few_queries_on_nested_fields

use_separate_tables_when:
  - schema_is_stable
  - need_referential_integrity
  - frequent_queries_on_nested_fields
  - need_complex_joins

-- 2. JSONB Indexes
CREATE INDEX idx_metadata_gin ON documents
  USING GIN (metadata jsonb_path_ops);

-- Fast containment queries:
SELECT * FROM documents
WHERE metadata @> '{"status": "published", "author": "john"}';

-- 3. JSONB Constraints
ALTER TABLE documents
ADD CONSTRAINT valid_metadata
  CHECK (jsonb_typeof(metadata) = 'object');

ALTER TABLE documents
ADD CONSTRAINT required_status
  CHECK (metadata ? 'status');

-- 4. Query Patterns
-- Extract field:
SELECT metadata->>'status' FROM documents;

-- Nested extraction:
SELECT metadata->'user'->>'name' FROM documents;

-- Array contains:
SELECT * FROM documents
WHERE metadata->'tags' @> '["postgres"]';
```

---

### ‚ùå GAP 4.3: Temporal Data

**Severidade**: üü¢ M√âDIO

**Problema**:
- Bi-temporal patterns n√£o mencionado
- Audit trail design ausente
- Historical queries n√£o documentado

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em data/temporal-data-patterns.md

-- Bi-Temporal Table (valid time + transaction time)
CREATE TABLE products_history (
  id UUID,
  valid_from TIMESTAMPTZ NOT NULL,
  valid_to TIMESTAMPTZ,
  transaction_time TIMESTAMPTZ NOT NULL DEFAULT NOW(),

  -- Product data
  name TEXT NOT NULL,
  price NUMERIC(10,2) NOT NULL,

  PRIMARY KEY (id, valid_from, transaction_time)
);

-- Time-travel Query: "What was product X on 2024-01-01?"
SELECT * FROM products_history
WHERE id = 'product-uuid'
  AND valid_from <= '2024-01-01'
  AND (valid_to IS NULL OR valid_to > '2024-01-01')
ORDER BY transaction_time DESC
LIMIT 1;

-- Audit Trail Pattern
CREATE TABLE audit_log (
  id BIGSERIAL PRIMARY KEY,
  table_name TEXT NOT NULL,
  record_id TEXT NOT NULL,
  operation TEXT NOT NULL,  -- INSERT, UPDATE, DELETE
  old_data JSONB,
  new_data JSONB,
  changed_by UUID REFERENCES auth.users(id),
  changed_at TIMESTAMPTZ DEFAULT NOW()
);

-- Trigger for automatic audit
CREATE OR REPLACE FUNCTION audit_trigger()
RETURNS TRIGGER AS $$
BEGIN
  INSERT INTO audit_log (table_name, record_id, operation, old_data, new_data, changed_by)
  VALUES (
    TG_TABLE_NAME,
    COALESCE(NEW.id, OLD.id)::text,
    TG_OP,
    CASE WHEN TG_OP != 'INSERT' THEN to_jsonb(OLD) END,
    CASE WHEN TG_OP != 'DELETE' THEN to_jsonb(NEW) END,
    auth.uid()
  );
  RETURN COALESCE(NEW, OLD);
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

---

### ‚ùå GAP 4.4: Data Types Decision Tree

**Severidade**: üü¢ M√âDIO

**Problema**:
- Sem guidance sobre escolha de data types
- UUID vs BIGSERIAL n√£o explicado
- TEXT vs VARCHAR n√£o justificado
- NUMERIC vs FLOAT n√£o discutido

**Solu√ß√£o Necess√°ria**:

```yaml
# ADICIONAR em data/postgres-data-types.md

primary_keys:
  uuid:
    pros:
      - globally_unique
      - no_coordination_needed
      - security_through_obscurity
    cons:
      - larger_size (16 bytes vs 8)
      - slower_indexes
      - random_inserts (no_locality)
    use_when:
      - distributed_system
      - public_facing_ids
      - merge_data_from_sources

  bigserial:
    pros:
      - smaller (8 bytes)
      - sequential (better_index_performance)
      - human_readable
    cons:
      - coordination_required
      - exposes_record_count
      - merge_conflicts
    use_when:
      - single_database
      - internal_ids
      - high_insert_volume

text_vs_varchar:
  text:
    - no_length_limit
    - postgre_recommended
    - use_by_default
  varchar_n:
    - explicit_limit
    - validation_in_db
    - use_when_business_rule

numeric_vs_float:
  numeric:
    - exact_precision
    - use_for: money, quantities
  float:
    - approximate
    - use_for: scientific, measurements
```

---

### ‚ùå GAP 4.5: Denormalization Strategy

**Severidade**: üü¢ M√âDIO

**Problema**:
- "Pragmatic normalization" √© vago
- Sem guidance sobre quando desnormalizar
- Materialized views strategy ausente
- Cache tables pattern n√£o documentado

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em data/denormalization-patterns.md

-- 1. Materialized View (aggregation cache)
CREATE MATERIALIZED VIEW user_stats AS
SELECT
  user_id,
  COUNT(*) AS post_count,
  COUNT(DISTINCT DATE(created_at)) AS active_days,
  MAX(created_at) AS last_post_at,
  AVG(likes_count) AS avg_likes
FROM posts
GROUP BY user_id;

CREATE UNIQUE INDEX ON user_stats(user_id);

-- Refresh strategy (pg_cron)
SELECT cron.schedule(
  'refresh-user-stats',
  '*/5 * * * *',  -- Every 5 minutes
  $$REFRESH MATERIALIZED VIEW CONCURRENTLY user_stats$$
);

-- 2. Computed Column (stored)
ALTER TABLE users ADD COLUMN
  post_count INTEGER GENERATED ALWAYS AS (
    (SELECT COUNT(*) FROM posts WHERE user_id = users.id)
  ) STORED;

-- 3. Trigger-based Denormalization
CREATE OR REPLACE FUNCTION update_user_post_count()
RETURNS TRIGGER AS $$
BEGIN
  IF TG_OP = 'INSERT' THEN
    UPDATE users SET post_count = post_count + 1
    WHERE id = NEW.user_id;
  ELSIF TG_OP = 'DELETE' THEN
    UPDATE users SET post_count = post_count - 1
    WHERE id = OLD.user_id;
  END IF;
  RETURN NULL;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER posts_count_trigger
  AFTER INSERT OR DELETE ON posts
  FOR EACH ROW EXECUTE FUNCTION update_user_post_count();
```

---

## 5. Security Deep Dive

### ‚úÖ Pontos Fortes

```yaml
Security Basics:
  - RLS by default ‚úÖ
  - Service role warnings ‚úÖ
  - SSL enforcement ‚úÖ
  - Secrets redaction ‚úÖ
  - Input validation mentioned ‚úÖ
```

### ‚ùå GAP 5.1: SQL Injection Prevention

**Severidade**: üü° ALTO

**Problema**:
- Prepared statements n√£o enfor√ßados
- Dynamic SQL sanitization n√£o documentado

**Solu√ß√£o**: Ver doc `data/sql-injection-prevention.md`

---

### ‚ùå GAP 5.2: RLS Bypass Risks

**Severidade**: üî¥ CR√çTICO

**Problema**:
- SECURITY DEFINER risks n√£o documentados
- Search path attacks n√£o mencionados
- Function privilege escalation n√£o explicado

**Solu√ß√£o Necess√°ria**:

```sql
-- ADICIONAR em data/rls-security-pitfalls.md

-- 1. SECURITY DEFINER Risks
CREATE OR REPLACE FUNCTION dangerous_function()
RETURNS void AS $$
BEGIN
  -- ‚ùå BAD: Dynamic SQL in SECURITY DEFINER
  EXECUTE 'DELETE FROM users WHERE id = ' || user_input;
  -- Can bypass RLS!
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- ‚úÖ GOOD: Use parameters
CREATE OR REPLACE FUNCTION safe_function(user_input UUID)
RETURNS void AS $$
BEGIN
  DELETE FROM users WHERE id = user_input;
  -- RLS still applies
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- 2. Search Path Attack Prevention
ALTER FUNCTION public.my_function()
  SET search_path = public, pg_temp;

-- 3. Explicit Schema Qualification
-- ‚ùå BAD
SELECT * FROM users;

-- ‚úÖ GOOD
SELECT * FROM public.users;
```

---

### ‚ùå GAP 5.3: Data Masking

**Severidade**: üü° ALTO

**Problema**:
- PII protection strategies ausentes
- Column-level encryption n√£o mencionado
- Audit logging n√£o implementado

**Solu√ß√£o**: Ver doc `data/data-masking-patterns.md`

---

### ‚ùå GAP 5.4: Rate Limiting

**Severidade**: üü¢ M√âDIO

**Problema**:
- DB-level rate limiting n√£o existe
- Brute force protection ausente

**Solu√ß√£o**: Ver doc `data/rate-limiting-patterns.md`

---

## 6. Performance & Scalability

### ‚úÖ O Que Est√° Correto

```yaml
Performance Tools:
  - EXPLAIN workflow ‚úÖ
  - Index design task ‚úÖ
  - Hot path analysis ‚úÖ
  - Query optimization ‚úÖ
```

### ‚ùå GAP 6.1: Query Performance Tuning

**Severidade**: üü° ALTO

**Problema**:
- Configuration tuning n√£o mencionado
- Planner statistics tuning ausente
- Cost-based optimizer tricks n√£o documentados

**Solu√ß√£o**: Ver `data/postgres-tuning-guide.md` (criar)

---

### ‚ùå GAP 6.2: Connection Pooling

**Severidade**: üî¥ CR√çTICO (duplicado de GAP 1.2)

Ver GAP 1.2 para solu√ß√£o completa.

---

### ‚ùå GAP 6.3: Caching Strategy

**Severidade**: üü° ALTO

**Problema**:
- Query result caching ausente
- Materialized view refresh n√£o documentado
- Redis integration patterns n√£o mencionado

**Solu√ß√£o**: Ver GAP 4.5 (Materialized Views)

---

### ‚ùå GAP 6.4: Scaling Strategy

**Severidade**: üü¢ M√âDIO

**Problema**:
- Read replicas strategy n√£o mencionado
- Write scaling (sharding) ausente
- Cache layers (Redis) n√£o discutido

**Solu√ß√£o Necess√°ria**:

```yaml
# ADICIONAR em data/postgres-scaling-strategies.md

scaling_decision_tree:
  stage_1:
    rps: "< 100 RPS"
    solution: "Single Supabase instance"
    cost: "$"

  stage_2:
    rps: "100-1k RPS"
    solution:
      - connection_pooler
      - read_replicas (if read-heavy)
      - optimized_indexes
    cost: "$$"

  stage_3:
    rps: "1k-10k RPS"
    solution:
      - all_from_stage_2
      - redis_cache
      - cdn_for_static
      - materialized_views
    cost: "$$$"

  stage_4:
    rps: "> 10k RPS"
    solution:
      - all_from_stage_3
      - sharding_strategy
      - specialized_stores (timeseries, etc)
      - multi_region
    cost: "$$$$"
```

---

## 7. Scorecard Final

| Categoria | Score | Justificativa |
|-----------|-------|---------------|
| **PostgreSQL Core** | 7/10 | ‚úÖ Bons princ√≠pios, ‚ö†Ô∏è falta vacuum, partitioning, query optimization |
| **Supabase Integration** | 6/10 | ‚úÖ RLS b√°sico OK, ‚ùå falta Realtime, Storage, Auth hooks completos |
| **Security (RLS)** | 7/10 | ‚úÖ KISS policies, ‚ùå falta multi-tenancy, time-based, hierarchical |
| **Operations/DBA** | 8/10 | ‚úÖ Forte em workflows, ‚ùå falta backup/recovery, zero-downtime |
| **Performance** | 6/10 | ‚úÖ EXPLAIN OK, ‚ùå falta connection pooling, caching, tuning |
| **Scalability** | 4/10 | ‚ùå N√£o documentado (read replicas, sharding, cache layers) |
| **Monitoring** | 3/10 | ‚ùå Quase ausente (pg_stat_statements, alerting, dashboards) |
| **Zero-Downtime** | 2/10 | ‚ùå N√£o implementado (expand/contract pattern) |
| **Disaster Recovery** | 1/10 | ‚ùå Ausente (backup, PITR, verification) |

**Overall: 6/10** - Bom fundamento, mas **N√ÉO production-ready** sem os gaps cr√≠ticos.

---

## 8. Roadmap de Corre√ß√µes

### üî¥ FASE 1: Gaps Cr√≠ticos (BLOQUEIAM PRODU√á√ÉO)

**Prioridade**: ‚ö†Ô∏è CR√çTICO
**Tempo Estimado**: 8-12 horas
**Impacto**: Sem isso, n√£o usar em produ√ß√£o

#### Tasks a Criar/Modificar:
1. ‚úÖ **Schema Version Tracking**
   - Modificar `db-apply-migration.md`
   - Criar `db-migration-verify.md`
   - Adicionar tabela `schema_migrations`

2. ‚úÖ **Zero-Downtime Migrations**
   - Criar `db-zero-downtime-migration.md`
   - Documentar expand/contract pattern
   - Exemplos: add, rename, change type, split

3. ‚úÖ **Backup/Disaster Recovery**
   - Criar `db-backup.md`
   - Criar `db-backup-verify.md`
   - Criar `db-restore.md`
   - Criar `db-restore-pit.md`

4. ‚úÖ **Monitoring Integration**
   - Modificar `db-monitor-queries.md`
   - Criar `db-monitoring-setup.md`
   - Criar `db-alert-thresholds.md`
   - Documentar pg_stat_statements

---

### üü° FASE 2: Gaps Alto (RISK MITIGATION)

**Prioridade**: üü° ALTO
**Tempo Estimado**: 12-16 horas
**Impacto**: Seguran√ßa e performance

#### Tasks a Criar/Modificar:
5. ‚úÖ **RLS Patterns Completos**
   - Expandir `rls-policies-tmpl.yaml`
   - Adicionar multi-tenancy
   - Adicionar time-based
   - Adicionar hierarchical
   - Adicionar storage objects

6. ‚úÖ **Connection Pooling**
   - Criar `data/postgres-connection-pooling.md`
   - Documentar transaction vs session
   - Pool sizing calculator
   - Quando usar pooler vs direct

7. ‚úÖ **Realtime Configuration**
   - Criar `db-realtime-setup.md`
   - Criar `db-realtime-monitor.md`
   - Publication management
   - WAL monitoring

8. ‚úÖ **Storage Integration**
   - Adicionar se√ß√£o em `rls-policies-tmpl.yaml`
   - Bucket security
   - Upload/download policies

---

### üü¢ FASE 3: Gaps M√©dio (OPTIMIZATION)

**Prioridade**: üü¢ M√âDIO
**Tempo Estimado**: 12-16 horas
**Impacto**: Performance e best practices

#### Data Files a Criar:
9. ‚úÖ `data/postgres-partitioning.md`
10. ‚úÖ `data/postgres-jsonb-patterns.md`
11. ‚úÖ `data/temporal-data-patterns.md`
12. ‚úÖ `data/postgres-data-types.md`
13. ‚úÖ `data/denormalization-patterns.md`
14. ‚úÖ `data/postgres-query-optimization.md`
15. ‚úÖ `data/postgres-tuning-guide.md`
16. ‚úÖ `data/postgres-scaling-strategies.md`

---

## 9. Recomenda√ß√µes Finais

### Para Uso Imediato (Development)

**Se voc√™ precisa usar AGORA em desenvolvimento**:
1. ‚úÖ Pode usar as tasks operacionais (snapshot, apply, rollback)
2. ‚úÖ Pode usar os templates de documenta√ß√£o
3. ‚ö†Ô∏è Documentar que √© dev-only, n√£o prod-ready

### Para Production

**Antes de usar em produ√ß√£o**, DEVE implementar:
1. ‚ùå Schema version tracking (GAP 1.1)
2. ‚ùå Backup/restore completo (GAP 3.2)
3. ‚ùå Monitoring b√°sico (GAP 3.3)
4. ‚ùå RLS patterns completos para seu use case (GAP 2.5)

**Opcionalmente** (baseado em necessidade):
- Zero-downtime se uptime cr√≠tico (GAP 3.1)
- Connection pooling se alta concorr√™ncia (GAP 1.2)
- Realtime se usar Supabase Realtime (GAP 2.1)

---

## 10. Pr√≥ximos Passos

### Passo 1: Validar Esta An√°lise
- [ ] Revisar an√°lise de gaps
- [ ] Confirmar prioridades
- [ ] Ajustar roadmap

### Passo 2: Auditar Arquivos Existentes
- [ ] Auditar `schema-design-tmpl.yaml`
- [ ] Auditar `rls-policies-tmpl.yaml`
- [ ] Auditar `migration-plan-tmpl.yaml`
- [ ] Auditar `index-strategy-tmpl.yaml`
- [ ] Auditar todas as 11 tasks

### Passo 3: Implementar Fase 1 (Cr√≠ticos)
- [ ] Implementar GAP 1.1 (Schema Version Tracking)
- [ ] Implementar GAP 3.1 (Zero-Downtime Migrations)
- [ ] Implementar GAP 3.2 (Backup/Recovery)
- [ ] Implementar GAP 3.3 (Monitoring)

### Passo 4: Testar em Projeto Real
- [ ] Aplicar em projeto de teste
- [ ] Validar workflows
- [ ] Documentar issues encontrados
- [ ] Iterar corre√ß√µes

### Passo 5: Production Release
- [ ] Mover para `.aios-core/`
- [ ] Documentar breaking changes
- [ ] Criar guia de migration
- [ ] Anunciar disponibilidade

---

**Mantido por**: Alan (Database Expert) + Winston (Architect)
**√öltima Atualiza√ß√£o**: 2025-10-27
**Status**: ‚úÖ An√°lise Completa - Aguardando Implementa√ß√£o de Corre√ß√µes
