# Epic 1: Enhanced Analysis - HEXACO & Multimodal

**Epic ID:** EPIC-1
**Status:** ðŸ“‹ Planning
**Priority:** P1 (SHOULD HAVE)
**Timeline:** Weeks 3-4
**Story Points:** 13
**Owner:** Dev Lead
**Dependencies:** Epic 0 (Foundation) - COMPLETE âœ…

---

## Epic Overview

**Goal:** Extend InnerLens Lite with HEXACO framework (6th dimension: Honesty-Humility), improved linguistic markers, and basic multimodal analysis patterns.

**Why This Matters:**
- **HEXACO adds critical dimension**: Honesty-Humility predicts unethical behavior, corruption, and manipulation better than Big Five
- **Improved markers**: Expand detection from ~30 patterns to 100+ patterns for higher accuracy
- **Multimodal patterns**: Extract behavioral signals from WhatsApp timing, emoji, and email formality

**Success Criteria:**
- âœ… User can run `*detect-traits-quick --framework hexaco`
- âœ… HEXACO scores generated with 75%+ confidence (6 traits: HEXACO)
- âœ… Same fragments.json reused (MIU architecture validated)
- âœ… Linguistic marker database expanded to 100+ patterns
- âœ… WhatsApp timing patterns analyzed (response time, activity hours)
- âœ… Emoji usage patterns detected (frequency, type distribution)
- âœ… Email formality scoring implemented

---

## Stories (5 stories, 13 points)

### Story 1.1: HEXACO Framework Integration (3 points)

**As a** user
**I want** to analyze personality using HEXACO (6 dimensions) instead of Big Five
**So that** I can detect Honesty-Humility (predicts ethical behavior, manipulation, corruption)

**Acceptance Criteria:**

**Part A: HEXACO Framework Knowledge Base**
- [ ] `data/frameworks/hexaco-framework.md` created (800+ lines)
- [ ] Content includes:
  - [ ] History: Ashton & Lee (2004), HEXACO-PI-R, lexical hypothesis extension
  - [ ] 6 trait definitions (HEXACO):
    - [ ] **H**onesty-Humility: Sincerity, fairness, greed-avoidance, modesty
    - [ ] **E**motionality: Fearfulness, anxiety, dependence, sentimentality
    - [ ] **X**traversion: Social self-esteem, social boldness, sociability, liveliness
    - [ ] **A**greeableness: Forgivingness, gentleness, flexibility, patience
    - [ ] **C**onscientiousness: Organization, diligence, perfectionism, prudence
    - [ ] **O**penness: Aesthetic appreciation, inquisitiveness, creativity, unconventionality
  - [ ] 24 facets total (4 per trait vs Big Five's 6 per trait)
  - [ ] Linguistic markers for each facet (keywords, phrases, patterns)
  - [ ] Scoring guidelines (intensity/confidence thresholds)
  - [ ] Research validity: 30+ countries, cross-cultural validation
  - [ ] Predictive power: Honesty-Humility predicts unethical behavior (r=0.30-0.40)
  - [ ] Comparison to Big Five: What's different? When to use HEXACO?

**Part B: HEXACO Task Workflow**
- [ ] `tasks/analyze-hexaco.md` created (500+ lines, 12-step workflow similar to Big Five)
- [ ] Workflow steps:
  - [ ] Load fragments.json (reuse MIUs from @fragment-extractor)
  - [ ] Load HEXACO framework KB
  - [ ] Batch processing (20-30 MIUs per batch)
  - [ ] Detection with intensity + confidence scoring
  - [ ] Threshold: intensity >= 0.40 AND confidence >= 0.60
  - [ ] Map to 6 traits + 24 facets
  - [ ] Generate scores (0-100) + confidence + evidence
  - [ ] Statistical validation (mean, std dev, outliers)
  - [ ] Quality checks (8-point validation)
  - [ ] Output: hexaco-raw.yaml

**Part C: HEXACO Quality Checklist**
- [ ] `checklists/hexaco-quality.md` created (300+ lines)
- [ ] 9 validation dimensions (same as Big Five but adapted for 6 traits)
- [ ] Validation outcomes: VALIDATED_HIGH/MEDIUM/PROVISIONAL/REJECTED
- [ ] Remediation guidance for HEXACO-specific failures

**Part D: HEXACO Profile Template**
- [ ] `templates/hexaco-profile.yaml` created (700+ lines)
- [ ] Complete schema:
  - [ ] 6 traits (HEXACO) with scores + confidence
  - [ ] 24 facets (4 per trait) with scores + confidence
  - [ ] Evidence quotes (3-5 per trait)
  - [ ] Validation section
  - [ ] Inline documentation
- [ ] Example filled template: Realistic data (not placeholders)

**Part E: Integration with Existing Pipeline**
- [ ] Update `tasks/detect-traits-quick.md` to support `--framework` parameter
  - [ ] `--framework bigfive` (default, existing behavior)
  - [ ] `--framework hexaco` (new, uses tasks/analyze-hexaco.md)
- [ ] Fragments reused: Same fragments.json for both frameworks
- [ ] Cost optimization: Only analysis step ($0.12), no re-extraction

**Definition of Done:**
- @psychologist can execute `tasks/analyze-hexaco.md`
- `*detect-traits-quick --framework hexaco` works end-to-end
- Same fragments.json reused (validates MIU architecture)
- HEXACO profile validated with VALIDATED_HIGH outcome (test data)
- Performance: <90s for analysis (same as Big Five)
- All content in English

**Technical Notes:**
- HEXACO Emotionality â‰  Big Five Neuroticism (inverted scale)
- HEXACO Agreeableness â‰  Big Five Agreeableness (narrower definition)
- Honesty-Humility is the key differentiator (not in Big Five)
- Use @quality-assurance for validation (same agent, different checklist)

---

### Story 1.2: Expanded Linguistic Marker Database (2 points)

**As a** psychologist agent
**I want** access to 100+ linguistic markers per trait
**So that** detection accuracy improves from 75% to 80%+ correlation with self-reports

**Acceptance Criteria:**
- [ ] `data/markers/bigfive-markers-v2.yaml` created (1500+ lines)
- [ ] For each Big Five trait:
  - [ ] 20+ keyword patterns (vs current ~6)
  - [ ] 10+ phrase patterns (multiword expressions)
  - [ ] 5+ syntactic patterns (e.g., "I never..." for low Conscientiousness)
  - [ ] 5+ semantic patterns (e.g., abstract concepts for Openness)
  - [ ] Intensity weights (0.0-1.0) for each marker
  - [ ] Context rules (when to apply, when to skip)
- [ ] `data/markers/hexaco-markers-v1.yaml` created (1500+ lines)
  - [ ] Same structure as Big Five markers
  - [ ] Special focus on Honesty-Humility markers (20+ patterns)
- [ ] Marker categories:
  - [ ] **Direct expressions**: "I am curious", "I love trying new things"
  - [ ] **Behavioral descriptions**: "I always finish what I start"
  - [ ] **Decision patterns**: "When I face X, I..."
  - [ ] **Emotional reactions**: "I get anxious when..."
  - [ ] **Value statements**: "I believe in...", "I prioritize..."
  - [ ] **Hypotheticals**: "I would never...", "If I could..."
  - [ ] **Negations**: "I'm not...", "I rarely..."
  - [ ] **Comparisons**: "More than most people, I..."
- [ ] Testing:
  - [ ] Run on 10 test subjects
  - [ ] Compare accuracy: v1 markers vs v2 markers
  - [ ] Target: +5% correlation improvement (75% â†’ 80%)

**Definition of Done:**
- 100+ markers per Big Five trait (500+ total)
- 100+ markers per HEXACO trait (600+ total)
- @psychologist loads markers from YAML files
- Accuracy improvement demonstrated (N=10 test)
- All markers in English

**Technical Notes:**
- Markers stored separately from framework KBs (reusability)
- YAML format for easy editing by non-programmers
- Include marker sources (e.g., "NEO-PI-R item 42")
- Validate against known personality inventories

---

### Story 1.3: Multimodal Pattern Analysis - WhatsApp (3 points)

**As a** user
**I want** to analyze WhatsApp conversation exports for behavioral patterns
**So that** personality analysis includes messaging behavior (timing, emoji, message length)

**Acceptance Criteria:**

**Part A: WhatsApp Parser**
- [ ] `agents/data-parser.md` created (400+ lines)
- [ ] New agent persona: Data format specialist, multimodal extraction expert
- [ ] Commands:
  - [ ] `*parse-whatsapp` - Parse WhatsApp export (TXT or JSON format)
  - [ ] `*parse-email` - Parse email (MBOX or EML format) - Story 1.4
  - [ ] `*parse-code` - Parse code commits (future - v1.2)
- [ ] WhatsApp parsing workflow:
  - [ ] Read export file (WhatsApp format: "[2025-01-15, 14:22:30] John: Message text")
  - [ ] Extract metadata:
    - [ ] Timestamp (date, time)
    - [ ] Sender
    - [ ] Message text
    - [ ] Media type (photo, video, audio, document, sticker)
    - [ ] Emoji used
    - [ ] Message length (chars, words)
  - [ ] Generate behavioral patterns:
    - [ ] Response time distribution (how fast do they reply?)
    - [ ] Activity hours (when are they most active?)
    - [ ] Message length distribution (short vs long messages)
    - [ ] Emoji frequency (high vs low usage)
    - [ ] Emoji type distribution (happy, sad, neutral)
    - [ ] Initiation rate (who starts conversations?)
    - [ ] Media sharing frequency
  - [ ] Output: whatsapp-patterns.json

**Part B: WhatsApp Pattern Mapping to Traits**
- [ ] `data/mappings/whatsapp-to-traits.yaml` created (500+ lines)
- [ ] Behavioral pattern â†’ Trait mappings:
  - [ ] **Fast response time** (< 5 min avg) â†’ High Extraversion (social engagement)
  - [ ] **Night owl activity** (22:00-02:00) â†’ High Openness (unconventional hours)
  - [ ] **Consistent daily messages** â†’ High Conscientiousness (routine adherence)
  - [ ] **High emoji usage** (>10% of messages) â†’ High Emotionality (HEXACO) / High Neuroticism (Big Five)
  - [ ] **Long messages** (>50 words avg) â†’ High Openness (complexity) OR High Conscientiousness (detail)
  - [ ] **High initiation rate** (>60%) â†’ High Extraversion (social proactivity)
  - [ ] **Formal language** (no slang, proper grammar) â†’ High Conscientiousness OR Low Extraversion
  - [ ] **Media sharing** (>20% of messages) â†’ High Extraversion (sharing behavior)
- [ ] Confidence modifiers:
  - [ ] Sample size: <100 messages (low confidence), 100-500 (medium), 500+ (high)
  - [ ] Time span: <1 week (low), 1-4 weeks (medium), 1+ month (high)
  - [ ] Conversation partners: 1 partner (low), 2-5 (medium), 5+ (high)

**Part C: Integration with Fragment Extractor**
- [ ] Update @fragment-extractor to accept multimodal input:
  - [ ] `*extract-fragments --input text.txt` (existing)
  - [ ] `*extract-fragments --input text.txt --whatsapp whatsapp.txt` (new)
  - [ ] Merges text-based MIUs + WhatsApp behavioral patterns
  - [ ] Output: fragments.json with `behavioral_patterns` section

**Part D: Integration with Psychologist**
- [ ] Update @psychologist to use behavioral patterns:
  - [ ] Load whatsapp-to-traits.yaml mappings
  - [ ] Combine linguistic markers + behavioral patterns
  - [ ] Weighted scoring: Text (70%) + Patterns (30%)
  - [ ] Adjust confidence: Multimodal = higher confidence

**Definition of Done:**
- @data-parser agent activates and parses WhatsApp exports
- @fragment-extractor accepts multimodal input
- @psychologist uses behavioral patterns in scoring
- Accuracy test: Text-only vs Text+WhatsApp (N=10)
- Target: +3% correlation improvement with multimodal data
- All content in English

**Technical Notes:**
- WhatsApp export formats: iOS (TXT) vs Android (TXT, slightly different)
- Privacy: Strip conversation content, keep only metadata patterns
- Emoji detection: Use Unicode ranges (U+1F600â€“U+1F64F, etc.)
- Timezone normalization for activity hours analysis

---

### Story 1.4: Multimodal Pattern Analysis - Email (3 points)

**As a** user
**I want** to analyze email conversations for formality and response patterns
**So that** professional communication style informs personality analysis

**Acceptance Criteria:**

**Part A: Email Parser (extends @data-parser from Story 1.3)**
- [ ] Update `agents/data-parser.md` with email parsing
- [ ] Email parsing workflow:
  - [ ] Read MBOX or EML files
  - [ ] Extract metadata:
    - [ ] Timestamp (sent date/time)
    - [ ] Sender/recipient
    - [ ] Subject line
    - [ ] Email body text
    - [ ] Signature presence
    - [ ] Attachments (count, types)
    - [ ] Thread depth (how many replies?)
  - [ ] Generate formality patterns:
    - [ ] Greeting type: Formal ("Dear Dr. Smith") vs Informal ("Hi John") vs None
    - [ ] Closing type: Formal ("Sincerely") vs Informal ("Cheers") vs None
    - [ ] Signature presence: Yes/No
    - [ ] Grammar correctness: Proper capitalization, punctuation
    - [ ] Sentence length: Average words per sentence
    - [ ] Paragraph structure: Single block vs organized paragraphs
    - [ ] Response time: Hours to respond to incoming emails
    - [ ] Email length: Short (<100 words) vs Long (>200 words)
  - [ ] Output: email-patterns.json

**Part B: Email Pattern Mapping to Traits**
- [ ] `data/mappings/email-to-traits.yaml` created (400+ lines)
- [ ] Formality pattern â†’ Trait mappings:
  - [ ] **High formality** (greeting + closing + signature) â†’ High Conscientiousness (professionalism)
  - [ ] **Fast response** (< 2 hours) â†’ High Conscientiousness (responsiveness) OR High Extraversion (engagement)
  - [ ] **Long emails** (>200 words) â†’ High Openness (complexity) OR High Conscientiousness (thoroughness)
  - [ ] **Short emails** (<50 words) â†’ Low Openness (directness) OR High time efficiency
  - [ ] **Organized paragraphs** â†’ High Conscientiousness (structure)
  - [ ] **Casual tone** â†’ High Agreeableness (warmth) OR Low Formality preference
  - [ ] **Attachment sharing** â†’ High Conscientiousness (documentation) OR collaborative style
  - [ ] **Subject line precision** â†’ High Conscientiousness (clarity)
- [ ] Confidence modifiers:
  - [ ] Sample size: <20 emails (low), 20-100 (medium), 100+ (high)
  - [ ] Context: Single recipient (low), 2-5 (medium), 5+ (high diversity)
  - [ ] Professional vs personal: Professional (higher formality baseline)

**Part C: Integration with Existing Pipeline**
- [ ] Update @fragment-extractor:
  - [ ] `*extract-fragments --input text.txt --email emails.mbox`
  - [ ] Merges text MIUs + email behavioral patterns
  - [ ] Output: fragments.json with `email_patterns` section
- [ ] Update @psychologist:
  - [ ] Load email-to-traits.yaml mappings
  - [ ] Combine linguistic + WhatsApp + email patterns
  - [ ] Weighted scoring: Text (60%) + WhatsApp (20%) + Email (20%)

**Definition of Done:**
- @data-parser parses email (MBOX, EML formats)
- @psychologist uses email formality patterns
- Accuracy test: Text-only vs Text+Email (N=10)
- Target: +2% correlation improvement with email data
- Privacy: Email content treated as PRIVATE classification
- All content in English

**Technical Notes:**
- MBOX format: Standard email archive (Thunderbird, Apple Mail export)
- EML format: Individual email files
- Formality scoring algorithm: Rule-based (greeting + closing + grammar + structure)
- Strip PII before storage (email addresses, phone numbers)

---

### Story 1.5: Confidence Scoring Improvements (2 points)

**As a** quality assurance agent
**I want** improved confidence calibration based on evidence quantity and diversity
**So that** users trust confidence scores (no overconfidence with limited data)

**Acceptance Criteria:**

**Part A: Confidence Calibration Algorithm**
- [ ] Update `agents/psychologist.md` with new confidence formula
- [ ] **Current formula** (v1.0):
  ```
  confidence = base_confidence (fixed by analyst)
  - Adjust down if MIUs < 3
  ```
- [ ] **New formula** (v1.1):
  ```
  confidence = base_detection_confidence
    Ã— evidence_quantity_modifier
    Ã— evidence_diversity_modifier
    Ã— source_diversity_modifier
    Ã— consistency_modifier

  Where:
  - base_detection_confidence: 0.0-1.0 (per detection)
  - evidence_quantity_modifier:
      < 3 MIUs: 0.50
      3-4 MIUs: 0.75
      5-9 MIUs: 0.90
      10+ MIUs: 1.00
  - evidence_diversity_modifier:
      All same facet: 0.80
      2-3 facets: 0.90
      4+ facets: 1.00
  - source_diversity_modifier:
      Single source (text only): 0.90
      Dual source (text + WhatsApp OR text + email): 1.00
      Triple source (text + WhatsApp + email): 1.10 (capped at 1.00 final)
  - consistency_modifier:
      High variance (conflicting signals): 0.70
      Moderate variance: 0.85
      Low variance (consistent): 1.00
  ```
- [ ] **Confidence ceiling**: Never exceed 0.95 (95%) for text-based analysis
- [ ] **Confidence floor**: Never go below 0.40 (40%) unless flagged as REJECTED

**Part B: Confidence Visualization in Output**
- [ ] Update `templates/bigfive-profile.yaml`:
  - [ ] Add confidence breakdown per trait:
    ```yaml
    openness:
      score: 85
      confidence: 0.82
      confidence_breakdown:
        base_detection: 0.88
        evidence_quantity: 1.00  # 10+ MIUs
        evidence_diversity: 0.90  # 3 facets
        source_diversity: 1.00    # Text + WhatsApp
        consistency: 1.00         # Low variance
        final_confidence: 0.82    # Product of all modifiers
    ```
- [ ] Update `templates/hexaco-profile.yaml` similarly

**Part C: Validation Updates**
- [ ] Update `checklists/bigfive-quality.md`:
  - [ ] New check: "Confidence calibration v1.1" (replaces v1.0 check)
  - [ ] Overconfidence detection:
    - [ ] FAIL if confidence > 0.70 with < 3 MIUs
    - [ ] FAIL if confidence > 0.90 with text-only (no multimodal)
    - [ ] FAIL if confidence > 0.95 for any text-based analysis
  - [ ] Underconfidence detection:
    - [ ] WARN if confidence < 0.70 with 10+ MIUs and multimodal data
- [ ] Update `checklists/hexaco-quality.md` similarly

**Part D: Testing & Validation**
- [ ] Confidence calibration test (N=20 subjects):
  - [ ] Collect self-reported Big Five scores
  - [ ] Run InnerLens analysis (text only, text+WhatsApp, text+email)
  - [ ] Plot: Confidence vs Absolute Error
  - [ ] Target: Lower confidence = higher error (calibrated)
  - [ ] Current v1.0 baseline: Correlation r = ? (unknown)
  - [ ] Target v1.1: Correlation r > 0.60 (moderate calibration)

**Definition of Done:**
- New confidence formula implemented in @psychologist
- Confidence breakdown shown in YAML outputs
- @quality-assurance validates using new calibration rules
- Calibration test shows r > 0.60 correlation (confidence â†” accuracy)
- All updates in English

**Technical Notes:**
- Confidence = "How likely is this score correct?"
- Well-calibrated: 80% confidence â†’ 80% of scores within Â±10 points of truth
- Poorly calibrated: 80% confidence â†’ only 50% of scores within Â±10 points
- Over-calibrated: High confidence with low accuracy (dangerous)
- Under-calibrated: Low confidence with high accuracy (missed opportunity)

---

## Dependencies

### External Dependencies
- **AIOS-FULLSTACK 4.0+** - Core framework
- **Anthropic Claude Sonnet 4 API** - LLM analysis
- **Epic 0: Foundation** - COMPLETE âœ… (MIU architecture, Big Five baseline)

### Internal Dependencies
- **Story 1.1 â†’ Story 1.5**: HEXACO must work before confidence improvements tested on HEXACO
- **Story 1.2 â†’ Stories 1.3, 1.4**: Expanded markers should be ready for multimodal integration
- **Story 1.3 â†’ Story 1.4**: WhatsApp parser foundation reused for email parser
- **Stories 1.3, 1.4 â†’ Story 1.5**: Multimodal data needed for source diversity confidence modifier

**Recommended order**: 1.2 â†’ 1.1 â†’ 1.3 â†’ 1.4 â†’ 1.5

### Blockers
- [ ] Epic 0 testing complete (to validate Big Five accuracy baseline)
- [ ] 10 test subjects with WhatsApp exports (for Story 1.3 validation)
- [ ] 10 test subjects with email archives (for Story 1.4 validation)
- [ ] 20 test subjects for confidence calibration (Story 1.5)

---

## Risks & Mitigation

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **HEXACO accuracy <75%** | Medium | High | Use Ashton & Lee validated markers, test on diverse samples, compare to HEXACO-PI-R questionnaire |
| **Multimodal adds noise** | Medium | Medium | A/B test: Text-only vs Text+Multimodal, ensure multimodal improves accuracy or keep optional |
| **Privacy concerns (WhatsApp/Email)** | Low | High | Strip conversation content, analyze metadata only, clear consent requirements documented |
| **Marker database too large** | Low | Low | Use lazy loading, cache frequently used markers, optimize YAML parsing |
| **Confidence formula too complex** | Medium | Low | Provide simple explanation in UI, visualize breakdown in output YAML |

---

## Testing Strategy

### Unit Tests
- [ ] HEXACO marker detection (30 test cases per trait = 180 tests)
- [ ] WhatsApp parser (10 format variations: iOS, Android, group chats)
- [ ] Email parser (10 format variations: MBOX, EML, threading)
- [ ] Confidence formula (edge cases: 0 MIUs, 100 MIUs, conflicting signals)

### Integration Tests
- [ ] End-to-end HEXACO: Text input â†’ HEXACO output
- [ ] End-to-end multimodal: Text + WhatsApp â†’ Big Five output
- [ ] End-to-end multimodal: Text + Email â†’ Big Five output
- [ ] End-to-end full: Text + WhatsApp + Email â†’ HEXACO output

### Validation Tests
- [ ] **HEXACO accuracy** (N=10):
  - [ ] Collect self-reported HEXACO-PI-R scores
  - [ ] Run InnerLens HEXACO analysis
  - [ ] Compute Pearson correlation per trait (target: r > 0.75)
- [ ] **Multimodal accuracy improvement** (N=10):
  - [ ] Text-only baseline: r = X
  - [ ] Text + WhatsApp: r = X + 3% (target)
  - [ ] Text + Email: r = X + 2% (target)
  - [ ] Text + WhatsApp + Email: r = X + 5% (target)
- [ ] **Confidence calibration** (N=20):
  - [ ] Plot: Confidence vs Absolute Error
  - [ ] Target: r > 0.60 (confidence calibrated to accuracy)

---

## Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| **HEXACO Accuracy** | 75%+ correlation | Pearson r with HEXACO-PI-R self-reports (N=10) |
| **Multimodal Improvement** | +3-5% correlation | Text-only vs Text+Multimodal (N=10) |
| **Marker Expansion** | 100+ markers/trait | Count in YAML files |
| **Confidence Calibration** | r > 0.60 | Correlation: Confidence â†” Accuracy (N=20) |
| **Performance** | <2 minutes | Total pipeline time (HEXACO with multimodal) |
| **Completion Rate** | 100% | All 5 stories done |

---

## Rollout Plan

### Week 3: HEXACO & Markers (Stories 1.1, 1.2)
- **Day 1-2:** HEXACO framework KB + task workflow
- **Day 3:** HEXACO quality checklist + profile template
- **Day 4-5:** Expanded linguistic marker database (Big Five + HEXACO)

### Week 4: Multimodal & Confidence (Stories 1.3, 1.4, 1.5)
- **Day 1-2:** @data-parser agent + WhatsApp parsing + WhatsApp-to-traits mapping
- **Day 3:** Email parsing + Email-to-traits mapping
- **Day 4:** Confidence scoring improvements + calibration testing
- **Day 5:** Integration testing + validation (N=20 subjects)

### Week 4 End: Demo
- Record 5-min video: Text + WhatsApp + Email â†’ HEXACO profile
- Share accuracy results: Text-only (75%) vs Multimodal (80%+)
- Publish confidence calibration analysis

---

## Definition of Done (Epic Level)

- [ ] All 5 stories completed and tested
- [ ] HEXACO framework working end-to-end (`*detect-traits-quick --framework hexaco`)
- [ ] Same fragments.json reused for Big Five and HEXACO (validates MIU architecture)
- [ ] Linguistic marker database expanded: 500+ Big Five markers, 600+ HEXACO markers
- [ ] WhatsApp parser working (@data-parser accepts TXT exports)
- [ ] Email parser working (@data-parser accepts MBOX, EML formats)
- [ ] Multimodal pipeline: Text + WhatsApp + Email â†’ validated profile
- [ ] Confidence scoring v1.1 implemented (4-factor formula)
- [ ] Accuracy validation:
  - [ ] HEXACO: 75%+ correlation (N=10)
  - [ ] Multimodal improvement: +3-5% correlation (N=10)
  - [ ] Confidence calibration: r > 0.60 (N=20)
- [ ] Performance: <2min total pipeline time (HEXACO + multimodal)
- [ ] Privacy documentation updated (WhatsApp, email metadata handling)
- [ ] All documentation in English

---

## Next Epic

**Epic 2: Cross-Framework Triangulation (Weeks 5-6)**
- Schwartz Values (10 universal values)
- VIA Character Strengths (24 strengths)
- Cross-framework consistency checks
- Multi-framework confidence weighting

---

**Epic Status:** ðŸ“‹ Planning
**Last Updated:** 2025-01-15
**Owner:** Dev Lead
**Dependencies:** Epic 0 (COMPLETE âœ…)
